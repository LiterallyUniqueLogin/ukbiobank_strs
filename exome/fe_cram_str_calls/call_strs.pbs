#!/bin/bash
#PBS -q condo 
#PBS -N call_exome_STRs
#PBS -l nodes=1
#PBS -l mem=50GB
#PBS -l walltime=2:00:00
#PBS -o /projects/ps-gymreklab/jmargoli/ukbiobank/exome/fe_cram_str_calls/output
#PBS -e /projects/ps-gymreklab/jmargoli/ukbiobank/exome/fe_cram_str_calls/output
#PBS -V
#PBS -M jmargoli@ucsd.edu
#PBS -m a

# failed with 25gb but succeeded with 50gb on the test str so use 50gb 
# estimated cost: $366
# 1247 strs (in the exome) * avg of 9.9 cores/condo job with 50gb * 11min52sec/job * $0.015/hr * 10jobs/str (@ 4k samples/job with 40k samples)
# sources of uncertainty:
# The timing and memory cap are based off of a randomly chosen STR. Will they be significantly more for the average STR?
# The timing is from a single run, maybe it was faster than average.
# While there is an average of 10 cores/condo job, if we keep getting assigned to memory-poor nodes then the cost could almost double.

cd "$UKB"/exome/fe_cram_str_calls/ || { echo "Can't get to $UKB/exome/fe_cram_str_calls/ ; exiting " ; exit 1 ; }

source ./call_strs.sh

