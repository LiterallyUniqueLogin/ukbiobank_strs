How to run an imputation

Before running any step, you must set the TMPDIR environment variable
This should point to a directory where scratch files can be written

Step 1:
For each chromosome
python launch_impute.py <run_name> \
	<directory containing callset chrN.vcf.gz files> \
	<directory containing the .sample file for the callset> \
	<chr number to impute>
        --readme <Description of the run (if this is the first call with this run name)>

This runs the imputation.
It first creates the directory structure
$UKB/str_imputed/<run_name>
                     /batches
                     /README
                     /vcfs
                     /batches/output (a log directory)
                     /batches/old (a log directory)
README contains the description you wrote in the launch_impute.py invocation
/batches contains intermediate results and run logs
/vcfs will contain the final output vcfs

launch_python then launches a <45min 4-core job for each thousand samples (so ~500 jobs for full UKB)
TSCC can only handle 1500 jobs per user at once, so only exectue this command 3 times at once
before waiting for jobs to complete.
While this is running for one chromosome, do not rerun it for the same chromosome!
This will cause the output files for that chromosome to be overwritten in unpredictable manners.
Once all the jobs for a chromosome are done, you should try launching imputation for that chromosome again.
This will cause all the jobs that failed for that chromosome to be rerun, or show you which jobs you need
to manually inspect for errors. Keep doing this after each full run for the chromosome is done
until no jobs are rerun.

The output files will look like batches/chrN_samples_X_to_Y.vcf.gz

For each chromosome, once this is done, you can proceed to step 2

--Steps two and three merge the output files into the final vcfs--

Step 2:
For each chromosome
launch_merge_within_region.sh <run_name> <chromosome number>

This launches ~50 1 processor jobs for the chromosome, each merging all samples
for ~ a 50th of the chromosome into one VCF. Each job will take less than a day.
This will create the files batches/chrN_pos_A_to_B.vcf.gz

This script doesn't do anything to mitigate failed jobs - 
check if any of the jobs fail, and if so, relaunch them all.

For each chromosome, once this is done, you can proceed to step 3

Step 3:
For each chromosome,
launch_concat_chr_regions.sh <run_name> <chromosome number>

This launches a single 1 processor job that should take less than a day (up to two)
It will produce the final output files batches/chrN.vcf.gz and batches/chrN.vcf.gz.tbi

You're done!



Extra details (only necessary for debugging):
launch_impute.py calls impute.pbs which calls cut_input_vcf.sh and run_beagle.sh
launch_merge_within_region.sh calls merge_within_region.pbs which calls list_batches_in_order.py
launch_concat_chr_regions.sh calls concat_chr_regions.pbs

You can run check_for_errors.sh <run_name> which will open up a vim file
with all the errors from any of the pbs log files for jobs launched by any of the steps

All the steps will put pbs log files <script_name>.o<run_number> and .e files in batches/output
Step 1 will put beagle <file_name>.log files in batches/
Relaunching a chromosome in step 1 will move log files for previously successful runs to batches/output
Relaunching a chromosome in step 1 will move .log, .vcf.gz and .vcf.gz.tbi files for previously failed runs to batches/old
Step 2 will put bcftools <file_name>.log files in batches/
Step 3 will put bcftools <file_name>.log files in vcfs/

