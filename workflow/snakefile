import collections
import csv
import glob
import itertools
import json
import pathlib
import os.path
import shutil
import tempfile
import time

import numpy as np
import pandas as pd
import polars as pl

import python_array_utils as utils
import phenotypes

chr_lens = np.genfromtxt(
    'misc_data/genome/chr_lens.txt',
    usecols=[1],
    skip_header=1,
    dtype=int
)

str_imputation_run_name = 'first_pass'

wildcard_constraints:
    chrom='[0-9]+',
    pos='[0-9]+',
    phenotype='[^/]+',
    ethnicity='white_brits|black|south_asian|chinese|irish|white_other',
    subpop_prefix='|subpop_',
    subpop='|[^/]+/'

def chr_expand(string):
    return [string.replace('CHR', str(chrom)) for chrom in range(1, 23)]

def pheno_expand(string):
    return [string.replace('PHENO', phenotype) for phenotype in phenotypes.phenotypes_in_use]

def assert_both_or_neither(a, b):
    if a == '' and b == '':
        return False
    elif a != '' and b != '':
        return True
    else:
        print(a, b)
        assert False

def subpop_assert(wildcards):
    return assert_both_or_neither(wildcards.subpop_prefix, wildcards.subpop)

def get_subpop_flag(wildcards, input):
    if wildcards.subpop == '':
        return ''
    assert wildcards.subpop[-1:] == '/'
    return f'--subpop {input.subpop}'

def get_subpop_input(wildcards):
    if wildcards.subpop == '':
        return []
    return f'sample_qc/subpops/{wildcards.subpop[:-1]}.txt'

################## Loading and collating data ############################

rule decompress_field:
    input:
        # TODO temp
        ancient('main_dataset/raw_data/ukb46781.enc_ukb'),
        ancient('main_dataset/raw_data/ukb46782.enc_ukb')
    output:
        data=protected("main_dataset/extracted_data/{field_name}_{field_id,[0-9]+}.txt"),
        readme=protected("main_dataset/extracted_data/{field_name}_{field_id}_README.txt")
    resources:
        time='05:00:00'
    shell:
        "main_dataset/decompress_trait.py {wildcards.field_name} {wildcards.field_id} "

other_ethnicities=['black', 'white_other', 'irish', 'south_asian', 'chinese']
all_ethnicities=other_ethnicities+['white_brits']

rule ethnic_sample_lists:
    input:
        white_brits='sample_qc/common_filters/ethnicity/white_brits.sample',
        ethnicities=expand(rules.decompress_field.output.data, field_name='ethnicity_self_report', field_id='21000')
    output:
        # must all go into the same directory
        all_files=expand(
            'sample_qc/common_filters/ethnicity/{ethnicity}.sample',
            ethnicity=other_ethnicities
        )
    resources:
        time='00:05:00'
    params:
        outdir=lambda wildcards, output: '/'.join(output.all_files[0].split('/')[:-1])
    shell:
        'sample_qc/scripts/ethnicity.py {params.outdir} {input.white_brits} {input.ethnicities} '

# doesn't filter for relatedness, only QC problems
rule filter_samples_for_ethnicity:
    input:
        start_file='sample_qc/common_filters/ethnicity/{ethnicity}.sample',
        remove_files=glob.glob("sample_qc/common_filters/remove/*sample"),
        subpop = get_subpop_input
    output:
        samples='sample_qc/{subpop_prefix}runs/{subpop}{ethnicity}/no_phenotype/combined.sample',
        readme='sample_qc/{subpop_prefix}runs/{subpop}{ethnicity}/no_phenotype/README.txt'
    params:
        is_subpop = subpop_assert,
        subpop_flag = get_subpop_flag
    resources:
        time='00:05:00',
    shell:
        'sample_qc/scripts/combine.py {output.samples} {output.readme} {input.start_file} {input.remove_files} '
        '{params.subpop_flag} '

rule load_shared_covars:
    input:
        "main_dataset/extracted_data/assessment_ages_21003.txt"
    output:
        shared_covars=protected("traits/shared_covars/shared_covars.npy"),
        assessment_ages=protected("traits/shared_covars/assessment_ages.npy"),
        readme=protected("traits/shared_covars/README.txt"),
        covar_names=protected("traits/shared_covars/covar_names.txt")
    resources:
        time='00:15:00',
        mem_gb=10
    shell:
        "traits/load_shared_covars.py"

def input_fields_for_phenotype(wildcards):
    phenotype = wildcards.phenotype
    if phenotype in phenotypes.pheno_descs:
        args = phenotypes.pheno_descs[phenotype]
    else:
        raise ValueError(f"Couldn't identify phenotype {phenotype}")
    fields = [
        f"main_dataset/extracted_data/{wildcards.phenotype}_{args.data_field_id}.txt",
        *[f"main_dataset/extracted_data/{covar_field_name}_{covar_field_id}.txt" for
          covar_field_name, covar_field_id in args.categorical_covars]
    ]
    if phenotypes.is_binary(phenotype):
        fields.extend([
            "main_dataset/extracted_data/year_of_birth_34.txt",
            "main_dataset/extracted_data/month_of_birth_52.txt",
            "main_dataset/extracted_data/date_of_death_40000.txt",
        ])
    return fields

def assert_known_phenotype(phenotype):
    if phenotype in phenotypes.pheno_descs:
        return True
    else:
        raise ValueError(f"Couldn't identify phenotype {phenotype}")

def load_phenotype_binary_flag(phenotype):
    unit = phenotypes.pheno_descs[phenotype].unit
    if not phenotypes.is_binary(phenotype):
        return ''
    elif unit == 'binary_date_first_reported':
        return ''
    elif unit == 'binary_0_1_neg_nan':
        return '--zero-one-neg-nan'
    else:
        raise ValueError("Unexpected binary unit type")

rule load_phenotype:
    input:
        "traits/shared_covars/assessment_ages.npy",
        input_fields_for_phenotype,
        samples = rules.filter_samples_for_ethnicity.output.samples,
        subpop = get_subpop_input
    output:
        phenotype=protected("traits/{subpop_prefix}phenotypes/{subpop}{ethnicity}/{phenotype}.npy"),
        readme=protected("traits/{subpop_prefix}phenotypes/{subpop}{ethnicity}/{phenotype}_README.txt"),
        unit=protected("traits/{subpop_prefix}phenotypes/{subpop}{ethnicity}/{phenotype}_unit.txt"),
        covar_names=protected("traits/{subpop_prefix}phenotypes/{subpop}{ethnicity}/{phenotype}_covar_names.txt")
    resources:
        time='00:05:00'
    params:
        known_phenotype = lambda wildcards: assert_known_phenotype(wildcards.phenotype),
        command = lambda wildcards: (
            'traits/load_continuous_phenotype_from_main_dataset.py'
            if not phenotypes.is_binary(wildcards.phenotype) else
            'traits/load_binary_phenotype_from_main_dataset.py'
        ),
        data_field_id = lambda wildcards: phenotypes.pheno_descs[wildcards.phenotype].data_field_id,
        unit_param = lambda wildcards: (
            f"'{phenotypes.pheno_descs[wildcards.phenotype].unit}'"
            if not phenotypes.is_binary(wildcards.phenotype) else
            ''
        ),
        binary_flag = lambda wildcards: load_phenotype_binary_flag(wildcards.phenotype),
        categorical_flag = lambda wildcards: (
            '' if not phenotypes.pheno_descs[wildcards.phenotype].categorical_covars else
            '--categorical-covars ' + " ".join(
                f'{name},{ID}' for name, ID in 
                phenotypes.pheno_descs[wildcards.phenotype].categorical_covars
            )
        ),
        is_subpop = subpop_assert,
        outprefix = lambda wildcards, output: output.phenotype.rsplit('.', maxsplit=1)[0]
    shell:
        '{params.command} '
        '{params.outprefix} '
        '{input.samples} '
        '{wildcards.ethnicity} '
        '{wildcards.phenotype} '
        '{params.data_field_id} '
        '{params.unit_param} '
        '{params.binary_flag} '
        '{params.categorical_flag} '

# TODO update for ethnicity and subpop
rule plot_phenotype_by_sex:
    input:
        "traits/shared_covars/shared_covars.npy",
        "traits/phenotypes/{phenotype}.npy",
        "traits/phenotypes/{phenotype}_unit.txt"
    output:
        "traits/phenotypes/{phenotype}_distribution_by_sex.png"
    resources:
        time='24:00:00',
        threads=5,
        mem_gb=10
    shell:
        'traits/plot_phenotype.py {wildcards.phenotype} sex'

# TODO update for ethnicity and subpop
rule plot_phenotype_by_age:
    input:
        "traits/phenotypes/{phenotype}.npy",
        "traits/phenotypes/{phenotype}_unit.txt"
    output:
        "traits/phenotypes/{phenotype}_distribution_by_age.png"
    resources:
        time='24:00:00',
        threads=15,
        mem_gb=30
    shell:
        'traits/plot_phenotype.py {wildcards.phenotype} age'

#subset to unrelated samples
rule subset_samples_for_phenotype:
    input:
        kinship_file="misc_data/ukbgene/ukb46122_rel_s488282.dat",
        pheno_file=rules.load_phenotype.output.phenotype,
        phenotype=( # only actually necessary for binary phenotypes
            lambda wildcards: [rules.load_phenotype.output.phenotype]
                if wildcards.phenotype != 'no_phenotype'
                else []
        )
    output:
        samples=protected("sample_qc/{subpop_prefix}runs/{subpop}{ethnicity}/{phenotype}/combined_unrelated.sample")
    params:
        binary_flag = (
            lambda wildcards, input:
                '' if wildcards.phenotype == 'no_phenotype' or not phenotypes.is_binary(wildcards.phenotype)
                else f'--binary-pheno {input.phenotype} '
        ),
        is_subpop = subpop_assert
    resources:
        time='24:00:00'
    shell:
        'sample_qc/scripts/unrelated_individuals.py '
        '{output.samples} '
        '{input.kinship_file} '
        '{input.pheno_file} '
        '{params.binary_flag} '

rule transform_phenotype_subset:
    input:
        pheno=rules.load_phenotype.output.phenotype,
        samples=rules.subset_samples_for_phenotype.output.samples,
    output:
        readme=protected("traits/{subpop_prefix}subset_transformed_phenotypes/{subpop}{ethnicity}/{phenotype}_README.txt"),
        phenotype=protected("traits/{subpop_prefix}subset_transformed_phenotypes/{subpop}{ethnicity}/{phenotype}.npy")
    resources:
        time='00:05:00'
    params:
        is_subpop = subpop_assert,
        binary_flag = lambda wildcards: '' if not phenotypes.is_binary(wildcards.phenotype) else '--binary',
        outprefix = lambda wildcards, output: output.phenotype.split('.')[0]
    shell:
        'traits/transform_traits.py '
        '{params.outprefix} ' 
        '{input.pheno} '
        '{input.samples} '
        '{wildcards.phenotype} '
        '{wildcards.ethnicity} '
        '{params.binary_flag} '

rule length_confusion:
    input:
        expand(
            rules.subset_samples_for_phenotype.output.samples,
            ethnicity='white_brits',
            phenotype='no_phenotype',
            subpop='',
            subpop_prefix=''
        ),
        'array_imputed/ukb46122_imp_chr1_v3_s487283.sample'
    output:
        'side_analyses/length_confusion/chr{chrom}.tab'
    resources:
        time='47:30:00',
        mem_gb = 4
    shell:
        'side_analyses/length_confusion/length_confusion.py {wildcards.chrom}'

##################### Running and Plotting Associations ###########################

sample_fname = 'microarray/ukb46122_hap_chr1_v2_s487314.sample'

def concatenate_csvs(output_file, input_files):
    with open(output_file + '.temp', 'w') as outfile:
        first = True
        for f in input_files:
            with open(f) as infile:
                first_line = True
                for line in infile:
                    if first_line and first:
                        first = False
                        first_line = False
                        outfile.write(line)
                        continue
                    elif first_line and not first:
                        first_line = False
                        continue
                    outfile.write(line)
    shutil.move(output_file + '.temp', output_file)

def regions(region_len):
    regions = []
    for chrom in range(1, 23):
        chr_len = chr_lens[chrom-1]
        for start in range(1, chr_len, region_len):
            if start + region_len - 1 > chr_len:
                end = chr_len
            else:
                end = start + region_len - 1
            regions.append(f'{chrom}_{start}_{end}')
    return regions

# no readme for conditional gwas, use readme of prep conditional input
rule write_my_gwas_readme:
    output:
        readme=protected('association/results/{phenotype}/my_{run_type,str|imputed_snp}{binary,|_linear|_logistic}/README.txt'),
        timestamp=protected('association/results/{phenotype}/my_{run_type}{binary}/time.stamp')
    resources:
        time='00:05:00'
    run:
        run_type = wildcards.run_type
        if run_type == 'str':
            command_run_type = 'strs'
            imputation_flag = f'--imputation-run-name {str_imputation_run_name}'
        else:
            assert run_type == 'imputed_snp'
            command_run_type = 'imputed-snps'
            imputation_flag = ''

        binary = wildcards.binary
        if binary == '':
            assert not phenotypes.is_binary(wildcards.phenotype)
            binary_flag = ''
        elif binary == '_linear':
            assert phenotypes.is_binary(wildcards.phenotype)
            binary_flag = '--binary linear'
        else:
            assert binary == '_logistic'
            assert phenotypes.is_binary(wildcards.phenotype)
            binary_flag = '--binary logistic'

        shell(
            'touch association/results/{wildcards.phenotype}/my_{wildcards.run_type}/time.stamp && '
            'association/my_regional_gwas.py '
            '{output.readme} '
            f'{command_run_type} '
            '{wildcards.phenotype} '
            '--readme '
            f'{imputation_flag} '
            f'{binary_flag} '
        )

def my_regional_gwas_mem_gb_req(wildcards, attempt):
    if wildcards.suffix != '_logistic':
        return 9
    else:
        return 40

# TODO update for new syntax
rule run_regional_my_imputed_snp_gwas:
    input:
        "traits/phenotypes/{phenotype}.npy",
        'association/results/{phenotype}/my_imputed_snp{suffix}/time.stamp',
        "traits/subset_transformed_phenotypes/{phenotype}.npy"
    output:
        "association/results/{phenotype}/my_imputed_snp{suffix,.*}/batches/chr{chrom}_{start}_{end}.tab"
    resources:
        time='24:00:00',
        mem_gb=my_regional_gwas_mem_gb_req
    run:
        cmd = (
            'association/my_regional_gwas.py imputed-snps {wildcards.phenotype} '
            '--region {wildcards.chrom}:{wildcards.start}-{wildcards.end} '
        )
        if wildcards.suffix == '':
            assert not phenotypes.is_binary(wildcards.phenotype)
        elif wildcards.suffix == '_linear':
            assert phenotypes.is_binary(wildcards.phenotype)
            cmd += '--binary linear '
        else:
            assert wildcards.suffix == '_logistic'
            assert phenotypes.is_binary(wildcards.phenotype)
            cmd += '--binary logistic '

        shell(cmd)

rule concatenate_and_move_my_imputed_snp_continuous_gwas_chr21:
    input:
        lambda wildcards:
            [f'association/results/{wildcards.phenotype}/my_imputed_snp/batches/chr{region}.tab' for
             region in regions(int(1e5)) if region[:3] == '21_']
    output:
        protected('association/plots/input/{phenotype}/my_imputed_snp_chr21_results.tab')
    resources:
        time='04:00:00'
    run:
        concatenate_csvs(output[0], input)

rule copy_my_imputed_snp_binary_linear_gwas_chr21_10000001_11000000:
    input:
        'association/results/{phenotype}/my_imputed_snp_linear/batches/chr21_10000001_11000000.tab'
    output:
        protected('association/plots/input/{phenotype}/my_imputed_snp_linear_chr21_10000001_11000000_results.tab')
    resources:
        time='00:05:00'
    shell:
        'cp {input[0]} {output[0]}'

rule move_my_imputed_snp_binary_logistic_gwas_chr21_10000001_11000000:
    input:
        'association/results/{phenotype}/my_imputed_snp_logistic/batches/chr21_10000001_11000000.tab'
    output:
        protected('association/plots/input/{phenotype}/my_imputed_snp_logistic_chr21_10000001_11000000_results.tab')
    resources:
        time='00:05:00'
    run:
        shutil.move(input[0], output[0])

rule run_str_spot_test:
    input:
        shared_covars=rules.load_shared_covars.output.shared_covars,
        untransformed_phenotype=rules.load_phenotype.output.phenotype,
        transformed_phenotype=rules.transform_phenotype_subset.output.phenotype,
        vcf=f"str_imputed/runs/{str_imputation_run_name}/vcfs/annotated_strs/chr{{chrom}}.vcf.gz",
        vcf_idx=f"str_imputed/runs/{str_imputation_run_name}/vcfs/annotated_strs/chr{{chrom}}.vcf.gz.tbi"
    output:
        result=protected('association/{subpop_prefix}spot_test/{subpop}{ethnicity}/{phenotype}/chr{chrom}_{pos}.tab'),
        readme=protected('association/{subpop_prefix}spot_test/{subpop}{ethnicity}/{phenotype}/chr{chrom}_{pos}_README.txt'),
    resources:
        time='00:20:00',
        mem_gb=lambda wildcards: 9 if not phenotypes.is_binary(wildcards.phenotype) else 40
    params:
        is_subpop = subpop_assert,
        binary_flag = lambda wildcards: '' if not phenotypes.is_binary(wildcards.phenotype) else '--binary logistic'
    shell:
        'association/my_regional_gwas.py '
        '{output.readme} '
        'strs '
        '{wildcards.phenotype} '
        '--readme '
        f'--imputation-run-name {str_imputation_run_name} '
        '{params.binary_flag} '
        ' ; '
        'association/my_regional_gwas.py '
        '{output.result} '
        'strs '
        '{wildcards.phenotype} '
        '--region {wildcards.chrom}:{wildcards.pos}-{wildcards.pos} '
        '--pheno-and-covars {input.transformed_phenotype} '
        '--shared-covars {input.shared_covars} '
        '--untransformed-phenotypes {input.untransformed_phenotype} '
        f'--imputation-run-name {str_imputation_run_name} '
        '{params.binary_flag} '

# TODO update for new syntax
rule run_regional_my_str_gwas:
    input:
        shared_covars=rules.load_shared_covars.output.shared_covars,
        untransformed_phenotype=expand(rules.load_phenotype.output.phenotype, ethnicity='white_brits', phenotype='{phenotype}', subpop='{subpop}', subpop_prefix='{subpop_prefix}'),
        run_timestamp=expand(rules.write_my_gwas_readme.output.timestamp, run_type='str', binary='{binary}', phenotype='{phenotype}'),
        transformed_phenotype=expand(rules.transform_phenotype_subset.output.phenotype, ethnicity='white_brits', phenotype='{phenotype}', subpop='{subpop}', subpop_prefix='{subpop_prefix}'),
        vcf=f"str_imputed/runs/{str_imputation_run_name}/vcfs/annotated_strs/chr{{chrom}}.vcf.gz",
        vcf_idx=f"str_imputed/runs/{str_imputation_run_name}/vcfs/annotated_strs/chr{{chrom}}.vcf.gz.tbi"
    output:
        results=temporary("association/results/{phenotype}/my_str{binary,|_linear|_logistic}/batches/chr{chrom}_{start}_{end}.tab")
    resources:
        time='12:00:00',
        mem_gb=my_regional_gwas_mem_gb_req
    run:
        cmd = (
            'association/my_regional_gwas.py strs {wildcards.phenotype} '
            '--region {wildcards.chrom}:{wildcards.start}-{wildcards.end} '
            f'--imputation-run-name {str_imputation_run_name} '
        )

        if wildcards.suffix == '':
            assert not phenotypes.is_binary(wildcards.phenotype)
        elif wildcards.suffix == '_linear':
            assert phenotypes.is_binary(wildcards.phenotype)
            cmd += '--binary linear '
        else:
            assert wildcards.suffix == '_logistic'
            assert phenotypes.is_binary(wildcards.phenotype)
            cmd += '--binary logistic '

        shell(cmd)

rule concatenate_my_str_gwas:
    input:
        lambda wildcards:
            [f'association/results/{wildcards.phenotype}/my_str{wildcards.suffix}/batches/chr{region}.tab' for
             region in regions(int(1e7))]
    output:
        results = protected('association/results/{phenotype}/my_str{suffix,|_linear}/results.tab')
    resources:
        time='04:00:00'
    run:
        concatenate_csvs(output[0], input)

rule prep_continuous_plink_input:
    input:
        'traits/shared_covars/shared_covars.npy',
        'traits/shared_covars/covar_names.txt',
        expand(rules.transform_phenotype_subset.output.phenotype, phenotype='{phenotype}', ethnicity='white_brits', subpop='{subpop}', subpop_prefix='{subpop_prefix}'),
        expand(rules.load_phenotype.output.covar_names, ethnicity='white_brits', phenotype='{phenotype}', subpop='{subpop}', subpop_prefix='{subpop_prefix}')
    output:
        'association/results/{phenotype}/plink_snp{suffix,|_linear}/input/transformed_phenotype_and_covars.tab'
    resources:
        time='00:30:00'
    run:
        cmd = 'association/prep_plink_input.py {wildcards.phenotype} '
        if phenotypes.is_binary(wildcards.phenotype):
            assert wildcards.suffix == '_linear'
            cmd += '--binary linear '
        else:
            assert wildcards.suffix == ''

        shell(cmd)

rule prep_binary_plink_input:
    input:
        'traits/shared_covars/shared_covars.npy',
        'traits/shared_covars/covar_names.txt',
        expand(rules.transform_phenotype_subset.output.phenotype, phenotype='{phenotype}', ethnicity='white_brits', subpop='{subpop}', subpop_prefix='{subpop_prefix}'),
        expand(rules.load_phenotype.output.covar_names, ethnicity='white_brits', phenotype='{phenotype}', subpop='{subpop}', subpop_prefix='{subpop_prefix}')
    output:
        'association/results/{phenotype}/plink_snp_logistic/input/transformed_phenotype_and_covars.tab'
    resources:
        time='00:30:00'
    shell:
        'association/prep_plink_input.py {wildcards.phenotype} --binary logistic '

rule timestamp_plink_run:
    output:
        'association/results/{phenotype}/plink_snp{suffix,.*}/time.stamp'
    resources:
        time='00:05:00'
    shell:
        'touch association/results/{wildcards.phenotype}/plink_snp{wildcards.suffix}/time.stamp'

rule run_linear_plink_gwas_parent:
    input:
        'association/results/{phenotype}/plink_snp{suffix}/input/transformed_phenotype_and_covars.tab',
        'association/results/{phenotype}/plink_snp{suffix}/time.stamp'
    resources:
        time='06:00:00',
        threads=28,
        mem_gb=56
    run:
        if wildcards.suffix == '':
            assert wildcards.rin == 'rin_'
            assert not phenotypes.is_binary(wildcards.phenotype)
        else:
            assert wildcards.rin == ''
            assert wildcards.suffix == '_linear'
            assert phenotypes.is_binary(wildcards.phenotype)

        shell('PHENOTYPE={wildcards.phenotype} CHROM={wildcards.chrom} SUFFIX={wildcards.suffix} association/plink_association.sh')

use rule run_linear_plink_gwas_parent as run_linear_binary_plink_gwas with:
    output:
        results=temporary('association/results/{phenotype}/plink_snp{suffix,_linear}/chrs/chr{chrom}/plink2.{phenotype}.glm.linear.done'),
        plinklog='association/results/{phenotype}/plink_snp{suffix,_linear}/chrs/chr{chrom}/plink2.log'

use rule run_linear_plink_gwas_parent as run_linear_rin_plink_gwas with:
    output:
        results=temporary('association/results/{phenotype}/plink_snp{suffix,}/chrs/chr{chrom}/plink2.rin_{phenotype}.glm.linear.done'),
        plinklog='association/results/{phenotype}/plink_snp{suffix,}/chrs/chr{chrom}/plink2.log'

rule run_logistic_plink_gwas:
    input:
        'association/results/{phenotype}/plink_snp_logistic/input/transformed_phenotype_and_covars.tab',
        'association/results/{phenotype}/plink_snp_logistic/time.stamp'
    output:
        temporary('association/results/{phenotype}/plink_snp_logistic/batches/chr{chrom}_{start}_{end}/plink2.{phenotype}.glm.logistic.hybrid.done')
    resources:
        time='12:00:00',
        threads=28,
        mem_gb=56
    shell:
        'PHENOTYPE={wildcards.phenotype} CHROM={wildcards.chrom} START={wildcards.start} '
        'END={wildcards.end} SUFFIX=_logistic association/plink_association.sh'

def concatenate_whole_genome_plink_gwas_input(wildcards):
    if not wildcards.suffix:
        rin = 'rin_'
    else:
        assert phenotypes.is_binary(wildcards.phenotype)
        assert wildcards.suffix == '_linear'
        rin = ''

    return [
        f'association/results/{wildcards.phenotype}/plink_snp{wildcards.suffix}/chrs/chr{chrom}/plink2.{rin}{wildcards.phenotype}.glm.linear.done' for
        chrom in range(1, 23)
    ]

rule concatenate_linear_plink_gwas:
    input:
        concatenate_whole_genome_plink_gwas_input
    output:
        results=protected('association/results/{phenotype}/plink_snp{suffix,|_linear}/results.tab')
    resources:
        time='04:00:00'
    run:
        if wildcards.suffix == '':
            assert not phenotypes.is_binary(wildcards.phenotype)
        else:
            assert wildcards.suffix == '_linear'
            assert phenotypes.is_binary(wildcards.phenotype)

        concatenate_csvs(output.results, input)

rule append_mfi_to_comparison_logistic_plink_run:
    input:
        'association/results/{phenotype}/plink_snp_logistic/batches/chr{chrom}_{start}_{end}/plink2.{phenotype}.glm.logistic.hybrid.done'
    output:
        'association/plots/input/{phenotype}/plink_snp_logistic_chr{chrom}_{start}_{end}_results_with_mfi.npy'
    resources:
        time='00:30:00',
        mem_gb = 10
    shell:
        'association/append_mfi_to_plink_snp.py {input[0]} {output[0]} --binary logistic '

# TODO double check these doesn't use subset_gwas_results files
rule compare_my_to_plink_continuous_gwas:
    input:
        "traits/phenotypes/white_brits/eosinophil_count_unit.txt",
        'association/plots/input/eosinophil_count/my_imputed_snp_chr21_results.tab',
        # it's okay to ask for plink snp for the entire genome because we'll want that anyway
        'association/plots/input/eosinophil_count/plink_snp_results_with_mfi.npy'
    output:
        manhattan='association/plots/continuous_my_imputed_snp_vs_plink.html'
    resources:
        time='00:30:00',
        mem_gb=50
    shell:
        'association/interactive_manhattan_plot.py {output.manhattan} eosinophil_count html --my-plink-comparison --chrom 21 '

rule compare_my_to_plink_continuous_gwas_figure_plink_only:
    input:
        "traits/phenotypes/white_brits/eosinophil_count_unit.txt",
        # it's okay to ask for plink snp for the entire genome because we'll want that anyway
        'association/plots/input/eosinophil_count/plink_snp_results_with_mfi.npy'
    output:
        manhattan='association/plots/continuous_plink_snp.png'
    resources:
        time='00:30:00',
        mem_gb=50
    shell:
        'association/interactive_manhattan_plot.py {output.manhattan} eosinophil_count png '
        '--my-plink-comparison --chrom 21 --only-plink '

rule compare_my_to_plink_continuous_gwas_figure_mine_only:
    input:
        "traits/phenotypes/white_brits/eosinophil_count_unit.txt",
        'association/plots/input/eosinophil_count/my_imputed_snp_chr21_results.tab',
    output:
        manhattan='association/plots/continuous_my_imputed_snp.png'
    resources:
        time='00:30:00',
        mem_gb=50
    shell:
        'association/interactive_manhattan_plot.py {output.manhattan} eosinophil_count png '
        '--my-plink-comparison --chrom 21 --only-mine'

rule compare_my_to_plink_binary_linear_gwas:
    input:
        "traits/phenotypes/white_brits/afib_and_flutter_I48_unit.txt",
        'association/plots/input/afib_and_flutter_I48/my_imputed_snp_linear_chr21_10000001_11000000_results.tab',
        # it's okay to ask for plink snp for the entire genome because we'll want that anyway
        'association/plots/input/afib_and_flutter_I48/plink_snp_linear_results_with_mfi.npy'
    output:
        manhattan='association/plots/binary_linear_my_imputed_snp_vs_plink.html'
    resources:
        time='00:30:00',
        mem_gb=50
    shell:
        'association/interactive_manhattan_plot.py {output.manhattan} afib_and_flutter_I48 html --my-plink-comparison --binary linear '
        '--chrom 21 --start 10000001 --end 11000000 '

rule compare_my_to_plink_binary_logistic_gwas:
    input:
        "traits/phenotypes/white_brits/afib_and_flutter_I48_unit.txt",
        'association/plots/input/afib_and_flutter_I48/my_imputed_snp_logistic_chr21_10000001_11000000_results.tab',
        plink_snp_fname='association/plots/input/afib_and_flutter_I48/plink_snp_logistic_chr21_10000001_11000000_results_with_mfi.npy'
    output:
        manhattan='association/plots/binary_logistic_my_imputed_snp_vs_plink.html'
    resources:
        time='00:30:00',
        mem_gb=50
    shell:
        'association/interactive_manhattan_plot.py {output.manhattan} afib_and_flutter_I48 html --my-plink-comparison --binary logistic '
        '--plink-snp-fname {input.plink_snp_fname} '
        '--chrom 21 --start 10000001 --end 11000000 '

def manhattan_inputs(wildcards):
    prefix = f'association/results/{wildcards.phenotype}'
    if wildcards.binary_type:
        runtype = wildcards.binary_type.replace(".", "_")
    else:
        runtype = ""
    return [
        f'{prefix}/my_str{runtype}/results.tab',
        f'{prefix}/plink_snp{runtype}/results.tab'
    ]

rule manhattan:
    input:
        "traits/phenotypes/white_brits/{phenotype}_unit.txt",
        manhattan_inputs
    output:
        manhattan='association/plots/{phenotype}{binary_type,|.linear|.logistic}.manhattan.{ext,[^.]+}'
    resources:
        time='00:30:00',
        mem_gb=50
    run:
        cmd = 'association/interactive_manhattan_plot.py {output.manhattan} {wildcards.phenotype} {wildcards.ext} '
        if not phenotypes.is_binary(wildcards.phenotype):
            assert wildcards.binary_type == ''
        else:
            assert wildcards.binary_type != ''
            cmd += f'--binary {wildcards.binary_type[1:]} '
        shell(cmd)

rule region_manhattan:
    input:
        "traits/phenotypes/white_brits/{phenotype}_unit.txt",
        expand(rules.concatenate_my_str_gwas.output.results, phenotype='{phenotype}', suffix=''),
        expand(rules.concatenate_linear_plink_gwas.output.results, phenotype='{phenotype}', suffix=''),
    output:
        manhattan='association/plots/{phenotype}{binary_type,|.linear|.logistic}.manhattan.chr{chrom}_{start,[^_]+}_{end,[^_]+}{pub,|_pub}.{ext}'
    resources:
        time='00:30:00',
        mem_gb=50
    params:
        binary_flag = lambda wildcards: '' if not phenotypes.is_binary(wildcards.phenotype) else '--binary {wildcards.binary_type[1:]}',
        pub_flag = lambda wildcards: '' if not wildcards.pub else '--publication'
    shell:
        'association/interactive_manhattan_plot.py {output.manhattan} {wildcards.phenotype} {wildcards.ext} '
        '--chrom {wildcards.chrom} '
        '--start {wildcards.start} '
        '--end {wildcards.end} '
        '{params.binary_flag} '
        '{params.pub_flag} '

def plot_white_brits_assoc_input(wildcards):
    if not phenotypes.is_binary(wildcards.phenotype):
        suffix = ''
    else:
        suffix = '_linear'
    full_genome_res = expand(rules.concatenate_my_str_gwas.output.results, phenotype=wildcards.phenotype, suffix=suffix)[0]
    if os.path.exists(full_genome_res):
        return full_genome_res
    else:
        return expand(rules.run_str_spot_test.output.result, ethnicity='white_brits', chrom=wildcards.chrom, pos=wildcards.pos, phenotype=wildcards.phenotype)

# These currently don't work with binary phenos
rule plot_ethnic_str_locus:
    input:
        assoc_result=rules.run_str_spot_test.output.result,
        pheno_data=rules.transform_phenotype_subset.output.phenotype
    output:
        svg='association/{subpop_prefix}locus_plots/{subpop}{ethnicity}/{phenotype}_{chrom}_{pos}_{dosage_fraction_threshold,[^_]+}{pub,|_pub}.svg',
        png='association/{subpop_prefix}locus_plots/{subpop}{ethnicity}/{phenotype}_{chrom}_{pos}_{dosage_fraction_threshold,[^_]+}{pub,|_pub}.png'
    resources:
        time='00:30:00'
    params:
        unit = lambda wildcards: phenotypes.pheno_descs[wildcards.phenotype].unit,
        outloc= lambda wildcards, output: output.svg[:-4],
        pub_flag = lambda wildcards: '' if not wildcards.pub else '--publication',
        is_subpop = subpop_assert
        #binary_flag=lambda wildcards: '' if not phenotypes.is_binary(wildcards.phenotype) else '--binary'
    shell:
        'association/plot_locus.py '
        '{params.outloc} '
        '{input.assoc_result} {input.pheno_data} '
        '{wildcards.chrom} {wildcards.pos} {wildcards.phenotype} '
        '{wildcards.dosage_fraction_threshold} '
        "--unit '{params.unit}' "
        '{params.pub_flag} '

rule plot_str_all_ethnicities:
    input:
        vcf=f'str_imputed/runs/{str_imputation_run_name}/vcfs/annotated_strs/chr{{chrom}}.vcf.gz',
        white_brits_assoc_results=expand(rules.concatenate_my_str_gwas.output.results, phenotype='{phenotype}', suffix='')[0],
        ethnic_assoc_result=expand(rules.run_str_spot_test.output.result, ethnicity=other_ethnicities, phenotype='{phenotype}', chrom='{chrom}', pos='{pos}', subpop='{subpop}', subpop_prefix='{subpop_prefix}'),
        pheno_data=expand(rules.transform_phenotype_subset.output.phenotype, ethnicity=all_ethnicities, phenotype='{phenotype}', subpop='{subpop}', subpop_prefix='{subpop_prefix}')
    output:
        svg='association/{subpop_prefix}locus_plots/{subpop}all/{phenotype}_{chrom}_{pos}_{dosage_cutoff,[0-9]+}{pub,|_pub}.svg',
        png='association/{subpop_prefix}locus_plots/{subpop}all/{phenotype}_{chrom}_{pos}_{dosage_cutoff,[0-9]+}{pub,|_pub}.png'
    resources:
        time='00:30:00'
    params:
        unit = lambda wildcards: phenotypes.pheno_descs[wildcards.phenotype].unit,
        outloc = lambda wildcards, output: output.svg[:-4],
        assoc_results_d = (lambda wildcards, input: json.dumps({
            'white_brits': input.white_brits_assoc_results,
            **{ethnicity: expand(rules.run_str_spot_test.output.result, ethnicity=ethnicity, phenotype=wildcards.phenotype, chrom=wildcards.chrom, pos=wildcards.pos)[0] for ethnicity in other_ethnicities}
        })),
        pheno_data_d = (lambda wildcards, input: json.dumps({
            **{ethnicity: expand(rules.transform_phenotype_subset.output.phenotype, ethnicity=ethnicity, phenotype=wildcards.phenotype)[0] for ethnicity in all_ethnicities}
        })),
        pub_flag = lambda wildcards: '' if not wildcards.pub else '--publication'
    shell:
        'association/plot_locus_all_ethnicities.py '
        '{params.outloc} '
        "{input.vcf} '{params.assoc_results_d}' '{params.pheno_data_d}' "
        '{wildcards.chrom} {wildcards.pos} {wildcards.phenotype} '
        '{wildcards.dosage_cutoff} '
        "--unit '{params.unit}' "
        '{params.pub_flag} '


################### Running and Plotting Conditional Associations ######################
#TODO implement/test binary for all conditional commands (including ones that had an attempt at supporting binary)

def imputed_SNPs_as_input(imputed_SNPs):
    out = ''
    prev_idx = 1
    underscore_count = 0
    for idx, char in enumerate(imputed_SNPs):
        if idx == 0:
            continue
        if char== '_':
            underscore_count += 1
            if underscore_count % 3 == 0:
                out += imputed_SNPs[prev_idx:idx] + ' '
                prev_idx=idx+1
    out += imputed_SNPs[prev_idx:] + ' '
    return out

rule prep_conditional_input:
    input:
        lambda wildcards: [] if not wildcards.PACSIN2_STRs else glob.glob('finemapping/PACSIN2/gts/*')
    output:
        npy     ='association/results/{phenotype}/conditional_inputs/chr{chrom}_STR{STRs,.*}__ISNP{imputed_SNPs,.*}__ASNP{PACSIN2_STRs,|__PACSIN2.+}.npy',
        varnames='association/results/{phenotype}/conditional_inputs/chr{chrom}_STR{STRs,.*}__ISNP{imputed_SNPs,.*}__ASNP{PACSIN2_STRs,|__PACSIN2.+}_varnames.txt',
        readme  ='association/results/{phenotype}/conditional_inputs/chr{chrom}_STR{STRs,.*}__ISNP{imputed_SNPs,.*}__ASNP{PACSIN2_STRs,|__PACSIN2.+}_README.txt'
    resources:
        time='00:30:00'
    params:
        imputed_snps_flag = lambda wildcards: (
            '--imputed-SNPs ' + imputed_SNPs_as_input(wildcards.imputed_SNPs)
            if wildcards.imputed_SNPs
            else ''
        ),
        strs_flags =  lambda wildcards: (
            f'--imputation-run-name {str_imputation_run_name} --STRs ' 
            + wildcards.STRs.replace('_', ' ')
            if wildcards.STRs
            else ''
        ),
        PACSIN2_flag =  lambda wildcards: (
            f'--PACSIN2-STRs ' 
            + wildcards.PACSIN2_STRs[9:].replace('_', ' ')
            if wildcards.PACSIN2_STRs
            else ''
        ),
        outprefix = lambda wildcards, output: output.npy.split('.')[0]
    shell:
        'association/prep_conditional_inputs.py '
        '{params.outprefix} '
        '{wildcards.phenotype} '
        '{wildcards.chrom} '
        '{params.strs_flags} '
        '{params.imputed_snps_flag} '
        '{params.PACSIN2_flag} '

rule run_conditional_my_str_gwas:
    input:
        shared_covars=rules.load_shared_covars.output.shared_covars,
        untransformed_phenotype=expand(rules.load_phenotype.output.phenotype, phenotype='{phenotype}', ethnicity='white_brits', subpop='{subpop}', subpop_prefix='{subpop_prefix}'),
        transformed_phenotype=expand(rules.transform_phenotype_subset.output.phenotype, phenotype='{phenotype}', ethnicity='white_brits', subpop='{subpop}', subpop_prefix='{subpop_prefix}'),
        conditional_covars=rules.prep_conditional_input.output.npy,
        sample_fname = sample_fname,
        vcf=f"str_imputed/runs/{str_imputation_run_name}/vcfs/annotated_strs/chr{{chrom}}.vcf.gz",
        vcf_idx=f"str_imputed/runs/{str_imputation_run_name}/vcfs/annotated_strs/chr{{chrom}}.vcf.gz.tbi"
    output:
        results='association/results/{phenotype}/my_str_conditional/chr{chrom}_{start}_{end}_STR{STRs,.*}__ISNP{imputed_SNPs,.*}__ASNP{PACSIN2_STRs,|__PACSIN2.+}.tab'
    params:
        binary_flag = lambda wildcards: '' if not phenotypes.is_binary(wildcards.phenotype) else '--binary logistic'
    resources:
        time='3:00:00',
        mem_gb=9,
        threads=4
    shell:
        'association/my_regional_gwas.py '
        '{output.results} '
        'strs '
        '{wildcards.phenotype} '
        '--region {wildcards.chrom}:{wildcards.start}-{wildcards.end} '
        '--pheno-and-covars {input.transformed_phenotype} '
        '--shared-covars {input.shared_covars} '
        '--untransformed-phenotypes {input.untransformed_phenotype} '
        '--conditional-covars {input.conditional_covars} '
        f'--imputation-run-name {str_imputation_run_name} '
        '{params.binary_flag} '

rule prep_conditional_plink_input:
    input:
        rules.prep_conditional_input.output.npy,
        rules.prep_conditional_input.output.varnames,
        rules.load_shared_covars.output.shared_covars,
        rules.load_shared_covars.output.covar_names,
        expand(rules.transform_phenotype_subset.output.phenotype, phenotype='{phenotype}', ethnicity='white_brits', subpop='{subpop}', subpop_prefix='{subpop_prefix}'),
        expand(rules.load_phenotype.output.covar_names, ethnicity='white_brits', phenotype='{phenotype}', subpop='{subpop}', subpop_prefix='{subpop_prefix}')
    output:
        result='association/results/{phenotype}/conditional_inputs/chr{chrom}_STR{STRs,.*}__ISNP{imputed_SNPs,.*}__ASNP{PACSIN2_STRs,|__PACSIN2.+}_plink.tab'
    params:
        condition = lambda wildcards, output: output.result.split('/')[-1].split('_plink')[0]
    resources:
        time='00:30:00'
    run:
        cmd = (
            'association/prep_plink_input.py {wildcards.phenotype} '
            '--conditional {params.condition} '
        )
        if phenotypes.is_binary(wildcards.phenotype):
            cmd += '--binary '
        shell(cmd)

rule run_continuous_conditional_plink_gwas:
    input:
        rules.prep_conditional_plink_input.output.result
    output:
        results='association/results/{phenotype}/plink_snp_conditional/chr{chrom}_{start}_{end}_STR{STRs,.*}__ISNP{imputed_SNPs,.*}__ASNP{PACSIN2_STRs,|__PACSIN2.+}/plink2.rin_{phenotype}.glm.linear.done'
    params:
        condition = lambda wildcards, output: 'STR' + output.results.split('/')[-2].split('STR', maxsplit=1)[1]
    resources:
        time='01:00:00',
        mem_gb=56,
        threads=28
    shell:
        'PHENOTYPE={wildcards.phenotype} CHROM={wildcards.chrom} '
        'CONDITIONAL={params.condition} '
        'START={wildcards.start} END={wildcards.end} '
        'association/plink_association.sh'

rule run_binary_conditional_plink_gwas:
    input:
        rules.prep_conditional_plink_input.output.result
    output:
        'association/results/{phenotype}/plink_snp_conditional/chr{chrom}_{start}_{end}_STR{STRs,.*}__ISNP{ISNPs,.*}__ASNP{PACSIN2_STRs,|__PACSIN2.+}/plink2.rin_{phenotype}.glm.logistic.hybrid.done'
    params:
        condition = lambda wildcards, output: 'STR' + output.results.split('/')[-2].split('STR', maxsplit=1)[1]
    resources:
        time='24:00:00',
        mem_gb=110,
        threads=28
    shell:
        'PHENOTYPE={wildcards.phenotype} CHROM={wildcards.chrom} '
        'CONDITIONAL={params.condition} '
        'START={wildcards.start} END={wildcards.end} '
        'BINARY=set '
        'association/plink_association.sh'

def manhattan_conditional_conditioned_STRs_param(wildcards):
    conditioned_STRs = ''
    if wildcards.STRs:
        conditioned_STRs = wildcards.STRs.replace('_', ' ')
    if wildcards.PACSIN2_STRs:
        conditioned_STRs += ' '
        conditioned_STRs += wildcards.PACSIN2_STRs[9:].replace('_', ' ')
    return conditioned_STRs


# currently doesn't work for binary phenotypes
rule manhattan_conditional:
    input:
        expand(rules.concatenate_my_str_gwas.output.results, phenotype='{phenotype}', suffix=''),
        expand(rules.concatenate_linear_plink_gwas.output.results, phenotype='{phenotype}', suffix=''),
        conditional_STR_results=rules.run_conditional_my_str_gwas.output.results,
        conditional_imputed_SNP_results=rules.run_continuous_conditional_plink_gwas.output.results,
    output:
        manhattan='association/plots/{phenotype}.manhattan.chr{chrom}_{start}_{end}_STR{STRs,.*}__ISNP{imputed_SNPs,.*}__ASNP{PACSIN2_STRs,|__PACSIN2.+}.{ext,html|png|svg}'
    params:
        unit=lambda wildcards: phenotypes.pheno_descs[wildcards.phenotype].unit,
        conditioned_imputed_SNPs = lambda wildcards: imputed_SNPs_as_input(wildcards.imputed_SNPs),
        conditioned_STRs = manhattan_conditional_conditioned_STRs_param
    resources:
        time='00:30:00',
        mem_gb=50
    shell:
        'association/interactive_manhattan_plot.py '
        '{output.manhattan} '
        '{wildcards.phenotype} {wildcards.ext} '
        ' --unit "{params.unit}" '
        '--conditioned-STRs {params.conditioned_STRs} '
        '--conditioned-imputed-SNPs {params.conditioned_imputed_SNPs} '
        '--conditional-STR-results {input.conditional_STR_results} '
        '--conditional-imputed-SNP-results {input.conditional_imputed_SNP_results} '
        '--chrom {wildcards.chrom} '
        '--start {wildcards.start} '
        '--end {wildcards.end} '

###################### Extracting Association Signals #######################

def generate_regions_input(wildcards):
    phenotype = wildcards.phenotype
    if not phenotypes.is_binary(phenotype):
        return {
            'strs': expand(rules.concatenate_my_str_gwas.output.results, phenotype='{phenotype}', suffix=''),
            'imputed_snps': expand(rules.concatenate_linear_plink_gwas.output.results, phenotype='{phenotype}', suffix=''),
        }
    else:
        return {
            'strs': expand(rules.concatenate_my_str_gwas.output.results, phenotype='{phenotype}', suffix='_linear'),
            'imputed_snps': expand(rules.concatenate_linear_plink_gwas.output.results, phenotype='{phenotype}', suffix='_linear'),
        }

checkpoint generate_regions:
    input:
        unpack(generate_regions_input)
    output:
        readme=protected('signals/regions/{phenotype}_README.txt'),
        table=protected('signals/regions/{phenotype}.tab')
    resources:
        time='00:30:00',
        mem_gb = 10
    shell:
        'signals/regions.py {wildcards.phenotype} {input.strs} {input.imputed_snps} {output.table} '

def generated_regions(phenotype):
    # TODO temp
    #checkpoints.generate_regions.get(phenotype=phenotype)
    df = pl.scan_csv(
        expand(rules.generate_regions.output.table, phenotype=phenotype)[0],
        sep='\t'
    ).filter('any_strs').collect()
    return [
        (chrom, start, end) for (chrom, start, end) in 
        zip(df['chrom'], df['start'], df['end'])
        # remove a few for being too slow for finemapping
        if not ((
            phenotype == 'urate' and
            chrom == 4 and
            start == 8165642 and
            end == 11717761
        ) or (
            phenotype == 'total_bilirubin' and
            chrom == 12 and
            start == 19976272 and
            end == 22524428
        )
    ]

def n_regions_no_strs(phenotype):
    checkpoints.generate_regions.get(phenotype=phenotype)
    return pl.scan_csv(
        expand(rules.generate_regions.output.table, phenotype=phenotype)[0],
        sep='\t'
    ).select((~pl.col('any_strs')).sum()).collect()[0, 0]

rule generate_peaks:
    input:
        strs=(lambda wildcards: expand(
            rules.concatenate_my_str_gwas.output.results,
            phenotype=wildcards.phenotype,
            suffix=('' if not phenotypes.is_binary(wildcards.phenotype) else '_linear')
        )),
        imputed_snps=(lambda wildcards: expand(
            rules.concatenate_linear_plink_gwas.output.results,
            phenotype=wildcards.phenotype,
            suffix=('' if not phenotypes.is_binary(wildcards.phenotype) else '_linear')
        )),
    output:
        readme=protected('signals/peaks/{phenotype}_{spacing}_{thresh}_README.txt'),
        peaks=protected('signals/peaks/{phenotype}_{spacing}_{thresh}.tab')
    resources:
        time='01:00:00'
    shell:
        'signals/peaks.py {wildcards.phenotype} {wildcards.spacing} {wildcards.thresh} '
        '{input.strs} {input.imputed_snps} '
        '{output.readme} {output.peaks} '

rule region_peak_comparison:
    input:
        'signals/regions/{phenotype}.tab',
        'signals/peaks/{phenotype}_250000_5e-8.tab'
    output:
        'signals/comparison/{phenotype}.done'
    resources:
        time='00:01:00'
    shell:
        'signals/comp_peaks_to_regions.py {wildcards.phenotype} '

rule peak_stats:
    input:
        'workflow/phenotypes.py',
        [fname for phenotype in phenotypes.phenotypes_in_use
         for fname in 
         (f'signals/peaks/{phenotype}_250000_5e-8.tab',
          f'signals/peaks/{phenotype}_250000_5e-9.tab',
          f'signals/peaks/{phenotype}_250000_5e-10.tab',
          f'signals/peaks/{phenotype}_1000000_5e-8.tab')]
    output:
        'signals/peak_stats_summary.txt',
    resources:
        time='00:05:00'
    shell:
        'signals/peak_stats.py > signals/peak_stats_summary.txt ; '

rule peak_manhattan:
    input:
        "traits/phenotypes/white_brits/{phenotype}_unit.txt",
        manhattan_inputs,
        'signals/peaks/{phenotype}_{spacing}_{thresh}.tab'
    output:
        manhattan='association/plots/{phenotype}{binary_type,|.linear|.logistic}.manhattan.peaks_{spacing}_{thresh}.{ext,[^.]+}'
    resources:
        time='00:30:00',
        mem_gb=50
    run:
        cmd = (
            'association/interactive_manhattan_plot.py {output.manhattan} {wildcards.phenotype} {wildcards.ext} '
            '--peaks-spacing {wildcards.spacing} --peaks-thresh {wildcards.thresh} '
        )
        if not phenotypes.is_binary(wildcards.phenotype):
            assert wildcards.binary_type == ''
        else:
            assert wildcards.binary_type != ''
            cmd += f'--binary {wildcards.binary_type[1:]} '
        shell(cmd)

rule linear_overview_manhattan:
    input:
        my_str_run_readme=expand(rules.write_my_gwas_readme.output.readme, run_type='str', phenotype='{phenotype}', conditional='', binary=''),
        my_str_results=(lambda wildcards: expand(
            rules.concatenate_my_str_gwas.output.results,
            phenotype=wildcards.phenotype,
            suffix=('' if not phenotypes.is_binary(wildcards.phenotype) else '_linear')
        )),
        plink_snp_log_file=expand(rules.run_linear_rin_plink_gwas.output.plinklog, chrom=21, suffix='', phenotype='{phenotype}'),
        plink_snp_results=(lambda wildcards: expand(
            rules.concatenate_linear_plink_gwas.output.results,
            phenotype=wildcards.phenotype,
            suffix=('' if not phenotypes.is_binary(wildcards.phenotype) else '_linear')
        )),
        peaks_fname=expand(rules.generate_peaks.output.peaks, spacing=10000000, thresh='5e-8', phenotype='{phenotype}')
    output:
        manhattan='association/plots/{phenotype}.overview.manhattan.png'
    resources:
        time='00:30:00',
        mem_gb=75
    shell:
        'association/overview_manhattan_plot.py '
        '{wildcards.phenotype} '
        '{output.manhattan} '
        '{input.my_str_run_readme} '
        '{input.my_str_results} '
        '{input.plink_snp_log_file} '
        '{input.plink_snp_results} '
        '10000000 '
        '5e-8 '
        '{input.peaks_fname} '

# ----------- rerunning binary logistic regression associaiton on
# ----------- binary linear regression association regions

def regional_plink_imputed_snp_logistic_regressions(wildcards):
    assert phenotypes.is_binary(wildcards.phenotype)

    phenotype = wildcards.phenotype

    target_files = []
    for chrom, start, stop in generated_regions(phenotype):
        target_files.append(
            f'association/results/{phenotype}/plink_snp_logistic/batches/chr{chrom}_{start}_{stop}/plink2.{phenotype}.glm.logistic.hybrid.done'
        )

    return target_files

rule concatenate_all_regions_plink_imputed_snp_logistic_gwas:
    input:
        regional_plink_imputed_snp_logistic_regressions
    output:
        protected('association/results/{phenotype}/plink_snp_logistic/results.tab')
    resources:
        time='04:00:00'
    run:
        concatenate_csvs(output[0], input)

def regional_my_str_logistic_regressions(wildcards):
    assert phenotypes.is_binary(wildcards.phenotype)

    phenotype = wildcards.phenotype

    target_files = []
    for chrom, start, stop in generated_regions(phenotype):
        target_files.append(
            f"association/results/{phenotype}/my_str_logistic/batches/chr{chrom}_{start}_{stop}.tab"
        )

    return target_files

rule concatenate_all_regions_my_str_logistic_gwas:
    input:
        regional_my_str_logistic_regressions
    output:
        protected('association/results/{phenotype}/my_str_logistic/results.tab')
    resources:
        time='04:00:00'
    run:
        concatenate_csvs(output[0], input)

##################### Finemapping ##########################

#--------------FINEMAP-----------------

rule finemap_write_input_variants:
    input:
        expand(rules.concatenate_my_str_gwas.output.results, phenotype='{phenotype}', suffix=''),
        expand(rules.concatenate_linear_plink_gwas.output.results, phenotype='{phenotype}', suffix=''),
        'finemapping/str_imp_snp_overlaps/chr{chrom}_to_filter.tab'
    output:
        readme=protected('finemapping/finemap_results/{phenotype}/{chrom}_{start,[0-9]+}_{end,[0-9]+}{PACSIN2,|_three_PACSIN2_STRs}/README.txt'),
        zfile=protected('finemapping/finemap_results/{phenotype}/{chrom}_{start,[0-9]+}_{end,[0-9]+}{PACSIN2,|_three_PACSIN2_STRs}/finemap_input.z'),
        master=protected('finemapping/finemap_results/{phenotype}/{chrom}_{start,[0-9]+}_{end,[0-9]+}{PACSIN2,|_three_PACSIN2_STRs}/finemap_input.master'),
    resources:
        time = '00:30:00'
    params:
        outdir=lambda wildcards, output: '/'.join(output.zfile.split('/')[:-1]),
        PACSIN2_flag = lambda wildcards: '' if not wildcards.PACSIN2 else '--three-PACSIN2-STRs'
    shell:
        'finemapping/finemap_write_input_variants.py '
        '{params.outdir} '
        '{wildcards.phenotype} {wildcards.chrom} {wildcards.start} '
        '{wildcards.end} '
        '{params.PACSIN2_flag} '

rule finemap_load_gts:
    input:
        expand(rules.subset_samples_for_phenotype.output.samples, phenotype='{phenotype}', ethnicity='white_brits', subpop='', subpop_prefix=''),
        zfile=rules.finemap_write_input_variants.output.zfile,
        pacsin2_gts=lambda wildcards: [] if not wildcards.PACSIN2 else glob.glob('finemapping/PACSIN2/gts/*')
    output:
        gts='finemapping/finemap_results/{phenotype}/{chrom}_{start,[0-9]+}_{end,[0-9]+}{PACSIN2,|_three_PACSIN2_STRs}/gts.h5',
        readme=protected('finemapping/finemap_results/{phenotype}/{chrom}_{start,[0-9]+}_{end,[0-9]+}{PACSIN2,|_three_PACSIN2_STRs}/load_gts_README.txt'),
    resources:
        mem_gb = 6,
        time = '02:00:00'
    shell:
        'finemapping/finemapping_load_gts.py '
        '{output.readme} '
        '{output.gts} '
        f'{str_imputation_run_name} '
        '{input.zfile} '
        '{wildcards.phenotype} '
        '{wildcards.chrom} '
        '{wildcards.start} '
        '{wildcards.end} '
        '--varname-header '

def finemap_corrs_time(wildcards, attempt):
    if attempt == 1:
        return '01:00:00'
    else:
        return '47:30:00'

rule finemap_calc_corrs:
    input:
        rules.finemap_load_gts.output.gts
    output:
        corrs='finemapping/finemap_results/{phenotype}/{chrom}_{start,[0-9]+}_{end,[0-9]+}{PACSIN2,|_three_PACSIN2_STRs}/lds.h5'
    resources:
        time = finemap_corrs_time,
        mem_gb = 4,
        attempt = lambda wildcards, attempt: attempt
    params:
        outdir=lambda wildcards, output: '/'.join(output.corrs.split('/')[:-1])
    shell:
        'finemapping/finemap_calc_corrs.py '
        '{params.outdir} '
        '{wildcards.phenotype} {wildcards.chrom} {wildcards.start} '
        '{wildcards.end} '

rule finemap_write_corrs_and_run:
    input:
        rules.finemap_write_input_variants.output.master,
        rules.finemap_write_input_variants.output.zfile,
        rules.finemap_calc_corrs.output.corrs
    output:
        lds='finemapping/finemap_results/{phenotype}/{chrom}_{start,[0-9]+}_{end,[0-9]+}{PACSIN2,|_three_PACSIN2_STRs}/all_variants.ld',
        snp_file=protected('finemapping/finemap_results/{phenotype}/{chrom}_{start,[0-9]+}_{end,[0-9]+}{PACSIN2,|_three_PACSIN2_STRs}/finemap_output.snp'),
        config=protected('finemapping/finemap_results/{phenotype}/{chrom}_{start,[0-9]+}_{end,[0-9]+}{PACSIN2,|_three_PACSIN2_STRs}/finemap_output.config')
    resources:
        time = finemap_corrs_time,
        mem_gb = 8,
        threads = 4,
        attempt = lambda wildcards, attempt: attempt
    params:
        outdir=lambda wildcards, output: '/'.join(output.snp_file.split('/')[:-1])
    shell:
        'finemapping/finemap_write_corrs_and_run.py '
        '{params.outdir} '

def all_finemap_outputs(phenotype):
    assert not phenotypes.is_binary(phenotype)
    return [
        f'finemapping/finemap_results/{phenotype}/{chrom}_{start}_{stop}/finemap_output.snp'
        for chrom, start, stop in generated_regions(phenotype)
    ]

rule finemap_all_phenotypes:
    input:
        itertools.chain.from_iterable(
            all_finemap_outputs(phenotype) for phenotype in phenotypes.phenotypes_in_use
        )

rule summarize_finemap_output:
    input:
        finemap_outputs = lambda wildcards: all_finemap_outputs(wildcards.phenotype)
    output:
        'finemapping/finemap_results/{phenotype}/summary/summary.txt',
        'finemapping/finemap_results/{phenotype}/summary/best_STR_contribs.tab',
        'finemapping/finemap_results/{phenotype}/summary/best_STR_ranks.tab',
        'finemapping/finemap_results/{phenotype}/summary/avg_causal_count.png',
        'finemapping/finemap_results/{phenotype}/summary/str_contrib_fractions.png',
        'finemapping/finemap_results/{phenotype}/summary/single_str_contrib_fractions.png',
        'finemapping/finemap_results/{phenotype}/summary/str_rank.png',
        all_STR_contribs = 'finemapping/finemap_results/{phenotype}/summary/all_STR_contribs.tab'
    params:
        finemap_dirs = lambda wildcards: [
            '/'.join(fname.split('/')[:-1])
            for fname in all_finemap_outputs(wildcards.phenotype)
        ]
    resources:
        time='00:30:00'
    run:
        shell(
            'finemapping/summarize_finemap_output.py '
            '{wildcards.phenotype} '
            'finemapping/finemap_results/{wildcards.phenotype}/summary '
            + str(n_regions_no_strs(wildcards.phenotype)) + 
            ' {params.finemap_dirs} '
        )

rule all_summaries:
    input:
        pheno_expand('finemapping/finemap_results/PHENO/summary/all_STR_contribs.tab')

rule manhattan_finemap:
    input:
        "traits/phenotypes/white_brits/{phenotype}_unit.txt",
        expand(rules.concatenate_my_str_gwas.output.results, phenotype='{phenotype}', suffix=''),
        expand(rules.concatenate_linear_plink_gwas.output.results, phenotype='{phenotype}', suffix=''),
        'signals/regions/{phenotype}.tab',
        lambda wildcards: all_finemap_outputs(wildcards.phenotype)
    output:
        manhattan='association/plots/{phenotype}.manhattan.FINEMAP.{ext}'
    resources:
        time='00:30:00',
        mem_gb=50
    shell:
        'association/interactive_manhattan_plot.py {output.manhattan} {wildcards.phenotype} {wildcards.ext} '
        '--finemap-signals '
        
#-------------------SuSiE----------------
def susie_load_time(wildcards, attempt):
    if attempt == 1:
        return '00:30:00'
    else:
        return '47:30:00'

rule susie_load_gts:
    input:
        rules.load_shared_covars.output.shared_covars,
        expand(rules.subset_samples_for_phenotype.output.samples, ethnicity='white_brits', phenotype='{phenotype}', subpop='', subpop_prefix=''),
        expand(rules.transform_phenotype_subset.output.phenotype, ethnicity='white_brits', phenotype='{phenotype}', subpop='', subpop_prefix=''),
        f'str_imputed/runs/{str_imputation_run_name}/vcfs/annotated_strs/chr{{chrom}}.vcf.gz',
        expand(rules.concatenate_linear_plink_gwas.output.results, phenotype='{phenotype}', suffix=''),
        expand(rules.concatenate_my_str_gwas.output.results, phenotype='{phenotype}', suffix=''),
        'finemapping/str_imp_snp_overlaps/chr{chrom}_to_filter.tab',
        pacsin2_gts=lambda wildcards: [] if not wildcards.PACSIN2 else glob.glob('finemapping/PACSIN2/gts/*')
    output:
        choose_vars_readme=protected('finemapping/susie_{hardcalls,|hardcall_}results/{phenotype}/{chrom}_{start,[0-9]+}_{end,[0-9]+}{PACSIN2,|_three_PACSIN2_STRs}/choose_vars_README.txt'),
        load_gts_readme=protected('finemapping/susie_{hardcalls,|hardcall_}results/{phenotype}/{chrom}_{start,[0-9]+}_{end,[0-9]+}{PACSIN2,|_three_PACSIN2_STRs}/load_gts_README.txt'),
        pheno='finemapping/susie_{hardcalls,|hardcall_}results/{phenotype}/{chrom}_{start,[0-9]+}_{end,[0-9]+}{PACSIN2,|_three_PACSIN2_STRs}/pheno_residuals.h5',
        gt='finemapping/susie_{hardcalls,|hardcall_}results/{phenotype}/{chrom}_{start,[0-9]+}_{end,[0-9]+}{PACSIN2,|_three_PACSIN2_STRs}/gt_residuals.h5',
        outcols=protected('finemapping/susie_{hardcalls,|hardcall_}results/{phenotype}/{chrom}_{start,[0-9]+}_{end,[0-9]+}{PACSIN2,|_three_PACSIN2_STRs}/colnames.txt')
    resources:
        #time = susie_load_time,
        time = '47:30:00',
        attempt = lambda wildcards, attempt: attempt,
        mem_gb = 4,
    params:
        hardcalls_flag = lambda wildcards: '--hardcalls' if wildcards.hardcalls else '',
        PACSIN2_flag = lambda wildcards: '' if not wildcards.PACSIN2 else '--three-PACSIN2-STRs'
    shell:
        'finemapping/susie_choose_vars.py '
        '{output.choose_vars_readme} '
        '{output.outcols} '
        '{wildcards.phenotype} '
        '{wildcards.chrom} '
        '{wildcards.start} '
        '{wildcards.end} '
        '{params.PACSIN2_flag} '
        ' ; '
        'finemapping/finemapping_load_gts.py '
        '{output.load_gts_readme} '
        '{output.gt} '
        f'{str_imputation_run_name} '
        '{output.outcols} '
        '{wildcards.phenotype} '
        '{wildcards.chrom} '
        '{wildcards.start} '
        '{wildcards.end} '
        '--residuals '
        '--pheno-file {output.pheno} '
        '{params.hardcalls_flag} '

rule susie_load_all_gts_mean_platelet_volume:
    input:
        [
            f'finemapping/susie_results/mean_platelet_volume/{chrom}_{start}_{stop}/gt_residuals.h5'
            for chrom, start, stop in generated_regions('mean_platelet_volume')
        ]

# mem backoff this way works - jobs fail in under 10 minutes due to oom before restarting
def susie_run_mem(wildcards, attempt):
    return 16 * attempt

def susie_mpv_light_regions():
    if susie_mpv_light_regions.cache:
        return susie_mpv_light_regions.cache
    for chrom, start, stop in generated_regions('mean_platelet_volume'):
        colfile = f'finemapping/susie_results/mean_platelet_volume/{chrom}_{start}_{stop}/colnames.txt'
        if not os.path.exists(colfile):
            continue
        with open(colfile) as f:
            n_vars = len(f.readlines())
            if n_vars > 1000:
                continue
        if (chrom, start, stop) in {
            (1, 153787623, 154553683),
            (11, 47107038, 48326643),
            (4, 57571805, 58032587),
            (5, 140185448, 141542287),
            (3, 168523039, 169284136)
        }:
            continue
        susie_mpv_light_regions.cache.add((chrom, start, stop))
    return susie_mpv_light_regions.cache
susie_mpv_light_regions.cache = set()

def susie_run_time(wildcards, attempt):
    if wildcards.phenotype == 'mean_platelet_volume':
        if (int(wildcards.chrom), int(wildcards.start), int(wildcards.end)) in set(susie_mpv_light_regions()):
            return '02:00:00'
        return '47:30:00'
    else:
        if attempt == 1:
            return '02:00:00'
        else:
            return '47:30:00'
    '''
    else:
        with open(f'finemapping/susie_results/{wildcards.phenotype}/{wildcards.chrom}_{wildcards.start}_{wildcards.end}/colnames.txt') as f:
            n_vars = len(f.readlines())
        if n_vars <= 1000:
            return '02:00:00'
        return '47:30:00'
    '''
        
def susie_outputs(dirname):
    return dict(
        lbf=f'{dirname}/lbf.tab',
        lbf_variable=f'{dirname}/lbf_variable.tab',
        sigma2=f'{dirname}/sigma2.txt',
        V=f'{dirname}/V.tab',
        converged=f'{dirname}/converged.txt',
        lfsr=f'{dirname}/lfsr.tab',
        requested_coverage=f'{dirname}/requested_coverage.txt',
        alpha=f'{dirname}/alpha.tab',
    )
    #also all the csN.txt files, just an unknown number of them
    #downstream jobs should consume alpha.tab

rule susie_run:
    input:
        pheno=rules.susie_load_gts.output.pheno,
        gt=rules.susie_load_gts.output.gt,
    output:
        **susie_outputs('finemapping/susie_{hardcalls,|hardcall_}results/{phenotype}/{chrom}_{start,[0-9]+}_{end,[0-9]+}{PACSIN2,|_three_PACSIN2_STRs}')
    resources:
        time = susie_run_time,
        mem_gb = susie_run_mem,
        attempt = lambda wildcards, attempt: attempt,
    params:
        L=50,
        max_iter=500,
        outdir=lambda wildcards, output: '/'.join(output.alpha.split('/')[:-1])
    conda:
        'envs/ukb_r.env.yml'
    shell:
        'Rscript finemapping/susie_run.r '
        '{params.outdir} '
        '{input.pheno} '
        '{input.gt} '
        '{params.L} '
        '{params.max_iter} '


'''
TODO deal with apply filter
rule susie_run_mvp_light_hardcalls:
    input:
        [
        expand(rules.susie_run.output.alpha, chrom=chrom, start=start, end=end, hardcalls='hardcall_', phenotype='mean_platelet_volume')[0]
        for chrom, start, end in susie_mpv_light_regions()
        ]
'''

rule susie_run_change_tol:
    input:
        pheno=expand(rules.susie_load_gts.output.pheno, phenotype='{phenotype}', chrom='{chrom}', start='{start}', end='{end}', hardcalls='', PACSIN2=''),
        gt=expand(rules.susie_load_gts.output.gt, phenotype='{phenotype}', chrom='{chrom}', start='{start}', end='{end}', hardcalls='', PACSIN2=''),
    output:
        **susie_outputs('finemapping/susie_results/{phenotype}_tol_{tol}/{chrom}_{start}_{end}')
    resources:
        time = susie_run_time,
        mem_gb = susie_run_mem,
        attempt = lambda wildcards, attempt: attempt,
    params:
        L=50,
        max_iter=500,
        outdir=lambda wildcards, output: '/'.join(output.alpha.split('/')[:-1])
    conda:
        'envs/ukb_r.env.yml'
    shell:
        'Rscript finemapping/susie_run.r '
        '{params.outdir} '
        '{input.pheno} '
        '{input.gt} '
        '{params.L} '
        '{params.max_iter} '
        '--tol {wildcards.tol} '

rule susie_run_change_snp_str_ratio:
    input:
        cols=expand(rules.susie_load_gts.output.outcols, phenotype='{phenotype}', chrom='{chrom}', start='{start}', end='{end}', hardcalls='', PACSIN2=''),
        pheno=expand(rules.susie_load_gts.output.pheno, phenotype='{phenotype}', chrom='{chrom}', start='{start}', end='{end}', hardcalls='', PACSIN2=''),
        gt=expand(rules.susie_load_gts.output.gt, phenotype='{phenotype}', chrom='{chrom}', start='{start}', end='{end}', hardcalls='', PACSIN2=''),
    output:
        **susie_outputs('finemapping/susie_results/{phenotype}_snp_str_ratio_{ratio}/{chrom}_{start,[0-9]+}_{end,[0-9]+}')
    resources:
        time = susie_run_time,
        mem_gb = susie_run_mem,
        attempt = lambda wildcards, attempt: attempt,
    params:
        L=50,
        max_iter=500,
        outdir=lambda wildcards, output: '/'.join(output.alpha.split('/')[:-1])
    conda:
        'envs/ukb_r.env.yml'
    shell:
        'Rscript finemapping/susie_run.r '
        '{params.outdir} '
        '{input.pheno} '
        '{input.gt} '
        '{params.L} '
        '{params.max_iter} '
        '--snp-p-over-str-p {wildcards.ratio} --varnames-file {input.cols}'

rule susie_run_change_residual_variance:
    input:
        pheno=expand(rules.susie_load_gts.output.pheno, phenotype='{phenotype}', chrom='{chrom}', start='{start}', end='{end}', hardcalls='', PACSIN2=''),
        gt=expand(rules.susie_load_gts.output.gt, phenotype='{phenotype}', chrom='{chrom}', start='{start}', end='{end}', hardcalls='', PACSIN2=''),
    output:
        **susie_outputs('finemapping/susie_results/{phenotype}_res_var_{res_var}/{chrom}_{start}_{end}')
    resources:
        time = susie_run_time,
        mem_gb = susie_run_mem,
        attempt = lambda wildcards, attempt: attempt,
    params:
        L=50,
        max_iter=500,
        outdir=lambda wildcards, output: '/'.join(output.alpha.split('/')[:-1])
    conda:
        'envs/ukb_r.env.yml'
    shell:
        'Rscript finemapping/susie_run.r '
        '{params.outdir} '
        '{input.pheno} '
        '{input.gt} '
        '{params.L} '
        '{params.max_iter} '
        '--residual-variance {wildcards.res_var} '

rule susie_run_change_prior_variance:
    input:
        pheno=expand(rules.susie_load_gts.output.pheno, phenotype='{phenotype}', chrom='{chrom}', start='{start}', end='{end}', hardcalls='', PACSIN2=''),
        gt=expand(rules.susie_load_gts.output.gt, phenotype='{phenotype}', chrom='{chrom}', start='{start}', end='{end}', hardcalls='', PACSIN2=''),
    output:
        **susie_outputs('finemapping/susie_results/{phenotype}_prior_var_{prior_var}/{chrom}_{start}_{end}')
    resources:
        time = susie_run_time,
        mem_gb = susie_run_mem,
        attempt = lambda wildcards, attempt: attempt,
    params:
        L=50,
        max_iter=500,
        outdir=lambda wildcards, output: '/'.join(output.alpha.split('/')[:-1])
    conda:
        'envs/ukb_r.env.yml'
    shell:
        'Rscript finemapping/susie_run.r '
        '{params.outdir} '
        '{input.pheno} '
        '{input.gt} '
        '{params.L} '
        '{params.max_iter} '
        '--scaled-prior-variance {wildcards.prior_var} '

'''
rule all_susie_light_tol_test_mpv:
    input:
        sum([
            expand(rules.susie_run_change_tol.output.alpha, tol=0.001, phenotype='mean_platelet_volume', chrom=chrom, start=start, end=end)
            for chrom, start, end in susie_mpv_light_regions()
        ], [])

rule all_susie_test_light_mpv:
    input:
        sum([
            expand(rules.susie_run_change_snp_str_ratio.output.alpha, ratio={1.5, 4}, phenotype='mean_platelet_volume', chrom=chrom, start=start, end=end) +
            expand(rules.susie_run_change_tol.output.alpha, tol=0.0001, phenotype='mean_platelet_volume', chrom=chrom, start=start, end=end) +
            expand(rules.susie_run_change_prior_variance.output.alpha, prior_var={0.2, 0.0005}, phenotype='mean_platelet_volume', chrom=chrom, start=start, end=end) +
            expand(rules.susie_run_change_residual_variance.output.alpha, res_var={.95, .8}, phenotype='mean_platelet_volume', chrom=chrom, start=start, end=end)
            for chrom, start, end in susie_mpv_light_regions()
        ], [])
'''

def all_susie_outputs(phenotype):
    assert not phenotypes.is_binary(phenotype)
    return [
        f'finemapping/susie_results/{phenotype}/{chrom}_{start}_{stop}/alpha.tab'
        for chrom, start, stop in generated_regions(phenotype)
    ]

rule susie_all_phenotypes:
    input:
        sum([
            all_susie_outputs(phenotype) for phenotype in phenotypes.phenotypes_in_use
        ], [])

'''
rule susie_find_reruns:
    input:
        rules.susie_all_phenotypes.input
    output:
        'finemapping/susie_results/susie_rerun_regions.txt'
    params:
        pheno_to_region_fname_json=json.dumps({
            pheno: expand(rules.generate_regions.output.table, phenotype=pheno)[0]
            for pheno in phenotypes.phenotypes_in_use
        }),
        pheno_to_susie_results_dir_json=json.dumps({
            pheno: '/'.join(expand(
                rules.susie_run.output.alpha,
                phenotype=pheno,
                chrom='{chrom}',
                start='{start}',
                end='{end}'
            )[0].split('/')[:-2]) # cut off the {chrom}_{start}_{end}/alpha.tab at the end
            for pheno in phenotypes.phenotypes_in_use
        })
    shell:
        'finemapping/susie_find_reruns.py '
        '{output[0]} '
        "'{params.pheno_to_region_fname_json}' "
        "'{params.pheno_to_susie_results_dir_json}' "
'''

rule summarize_susie_output:
    input:
        rules.susie_all_phenotypes.input
    output:
        table='finemapping/susie_results/summary.tab'
    params:
        json_dirs_list = lambda _ :
            json.dumps([
                [
                    phenotype,
                    f'{chrom}_{start}_{stop}',
                    '/'.join(expand(
                        rules.susie_run.output.alpha, phenotype=phenotype, chrom=chrom,
                        start=start, end=stop
                    )[0].split('/')[:-1])
                ] for phenotype in phenotypes.phenotypes_in_use
                for chrom, start, stop in generated_regions(phenotype)
            ])
    resources:
        time='1:00:00'
    run:
        with tempfile.NamedTemporaryFile(mode='w') as dirs_file:
            dirs_file.write(params.json_dirs_list)
            dirs_file.write('\n')
            shell(
                f"finemapping/summarize_susie_results.py {dirs_file.name} {output.table} "
            )

# ----- finemapping summaries ----------
rule sort_gencode:
    input:
        'misc_data/gencode/gencode.v38lift37.annotation.gff3'
    output:
        temporary('misc_data/gencode/gencode.v38lift37.annotation.without_chr.gff3'),
        'misc_data/gencode/gencode.v38lift37.annotation.without_chr.sorted.gff3'
    resources:
        time='00:30:00'
    shell:
        "sed -e 's/^chr//' {input[0]} > {output[0]} ; "
        'bedtools sort -i {output[0]} > {output[1]} '

rule gencode_extract_feature:
    input:
        'misc_data/gencode/gencode.v38lift37.annotation.without_chr.sorted.gff3'
    output:
        'misc_data/gencode/gencode.v38lift37.annotation.without_chr.sorted.{feature_type}.gff3'
    resources:
        time='00:30:00'
    shell:
        "awk '{{ if ($3 == \"{wildcards.feature_type}\") {{ print $0 ; }} }}' {input[0]} > {output[0]} "

cut_str_gencode_command = "cut -d $'\\t' -f 1-3,11-13,15- "

rule str_closest_updownstream_feature:
    input:
        vcf='snpstr/flank_trimmed_vcf/chr{chrom}.vcf.gz',
        index_file='snpstr/info_field/chr{chrom}.vcf.gz.tbi',
        gencode_feature='misc_data/gencode/gencode.v38lift37.annotation.without_chr.sorted.{feature_type}.gff3'
    output:
        'side_analyses/str_annotations/closest{direction,|_upstream|_downstream}{coding,|_protein_coding}_{feature_type,exon|CDS|UTR|five_prime_UTR|three_prime_UTR|transcript|gene}{support,|_support_[1-5]}/chr{chrom}.tab'
    resources:
        time='00:30:00'
    run:
        if wildcards.direction == '_upstream':
            stream_opt = ' -D b -id '
        elif wildcards.direction == '_downstream':
            stream_opt = ' -D b -iu '
        else:
            assert wildcards.direction == ''
            stream_opt = ' -d '

        if wildcards.coding != '':
            grep_cmd = " | grep 'gene_type=protein_coding' "
        else:
            grep_cmd = ''

        if wildcards.support != '':
            supports = " ".join(str(x) for x in range(1, int(wildcards.support.split('_')[-1]) + 1))
            b_file = (
                "for level in " + supports + " ; do "
                'grep "transcript_support_level=${{level}}" {input.gencode_feature} '
                + grep_cmd +
                " ; done | sort -k 1,1 -k 4,4n "
            )
        else:
            b_file = 'cat {input.gencode_feature} ' + grep_cmd

        shell(
            'bedtools closest -t first '
            + stream_opt +
            '-a {input.vcf} -b <( '
            + b_file +
            " ) | "
             + cut_str_gencode_command +
             '> {output[0]}'
         )

rule str_nearby_features:
    input:
        vcf='snpstr/flank_trimmed_vcf/chr{chrom}.vcf.gz',
        index_file='snpstr/info_field/chr{chrom}.vcf.gz.tbi',
        gencode_feature='misc_data/gencode/gencode.v38lift37.annotation.without_chr.sorted.{feature_type}.gff3'
    output:
        'side_analyses/str_annotations/nearby_{feature_type}_d_{distance}/chr{chrom}.tab'
    resources:
        time='00:30:00'
    shell:
        ('bedtools window -w {wildcards.distance} -a {input.vcf} -b {input.gencode_feature} | ' +
         cut_str_gencode_command +
         '> {output[0]}')

rule str_intersects_feature:
    input:
        vcf='snpstr/flank_trimmed_vcf/chr{chrom}.vcf.gz',
        index_file='snpstr/info_field/chr{chrom}.vcf.gz.tbi',
        gencode_feature='misc_data/gencode/gencode.v38lift37.annotation.without_chr.sorted.{feature_type}.gff3'
    output:
        'side_analyses/str_annotations/intersects{coding,|_protein_coding}_{feature_type,exon|CDS|UTR|five_prime_UTR|three_prime_UTR|transcript|gene}{support,|_support_[1-5]}/chr{chrom}.tab'
    resources:
        time='00:30:00'
    run:
        if wildcards.coding != '':
            grep_cmd = " | grep 'gene_type=protein_coding' "
        else:
            grep_cmd = ''

        if wildcards.support != '':
            supports = " ".join(str(x) for x in range(1, int(wildcards.support.split('_')[-1]) + 1))
        else:
            supports = '1 2 3 4 5 NA' #anything goes

        shell(
            "bedtools intersect -wo -a {input.vcf} -b "
            "<(for level in " + supports + " ; do "
                'grep "transcript_support_level=${{level}}" {input.gencode_feature} '
                + grep_cmd +
            " ; done | sort -k 1,1 -k 4,4n ) | "
            + cut_str_gencode_command +
            '> {output[0]}'
         )

rule enrichments:
    input:
        'snpstr/flank_trimmed_vcf/vars.tab',
        'snpstr/str_loci.txt',
        'snpstr/repeat_units.tab',
        'misc_data/stalling/canon_structure.tab',
        chr_expand('export_scripts/intermediate_results/chrCHR_loci_summary.tab'),
        'misc_data/eSTR/eSTRs.csv',
        'misc_data/gencode/gencode.v38lift37.annotation.without_chr.sorted.gene.gff3',
        chr_expand('side_analyses/str_annotations/intersects_protein_coding_CDS_support_2/chrCHR.tab'),
        chr_expand('side_analyses/str_annotations/intersects_protein_coding_UTR_support_2/chrCHR.tab'),
        chr_expand('side_analyses/str_annotations/intersects_protein_coding_five_prime_UTR_support_2/chrCHR.tab'),
        chr_expand('side_analyses/str_annotations/intersects_protein_coding_three_prime_UTR_support_2/chrCHR.tab'),
        chr_expand('side_analyses/str_annotations/intersects_transcript_support_2/chrCHR.tab'),
        chr_expand('side_analyses/str_annotations/closest_downstream_protein_coding_exon_support_2/chrCHR.tab'),
        chr_expand('side_analyses/str_annotations/closest_upstream_protein_coding_exon_support_2/chrCHR.tab'),
        chr_expand('side_analyses/str_annotations/closest_downstream_protein_coding_gene/chrCHR.tab'),
        chr_expand('side_analyses/str_annotations/closest_upstream_protein_coding_gene/chrCHR.tab'),
        pheno_expand('association/results/PHENO/my_str/results.tab'),
        pheno_expand('finemapping/finemap_results/PHENO/summary/all_STR_contribs.tab')
    output:
        'side_analyses/str_annotations/all_STRs.tab',
        'post_finemapping/results/enrichments.tab',
        'post_finemapping/results/upstream_gene_dist_cdf_50kbp.png',
        'post_finemapping/results/upstream_gene_dist_cdf.png',
        'post_finemapping/results/downstream_gene_dist_cdf_50kbp.png',
        'post_finemapping/results/downstream_gene_dist_cdf.png',
        'post_finemapping/results/upstream_exon_dist_cdf_50kbp.png',
        'post_finemapping/results/upstream_exon_dist_cdf.png',
        'post_finemapping/results/downstream_exon_dist_cdf_50kbp.png',
        'post_finemapping/results/downstream_exon_dist_cdf.png',
        'post_finemapping/results/mean_len_cdf.png',
        'post_finemapping/results/upstream_gene_dist_cdf_50kbp.pdf',
        'post_finemapping/results/upstream_gene_dist_cdf.pdf',
        'post_finemapping/results/downstream_gene_dist_cdf_50kbp.pdf',
        'post_finemapping/results/downstream_gene_dist_cdf.pdf',
        'post_finemapping/results/upstream_exon_dist_cdf_50kbp.pdf',
        'post_finemapping/results/upstream_exon_dist_cdf.pdf',
        'post_finemapping/results/downstream_exon_dist_cdf_50kbp.pdf',
        'post_finemapping/results/downstream_exon_dist_cdf.pdf',
        'post_finemapping/results/mean_len_cdf.pdf',
        'post_finemapping/results/FINEMAP_FM_eSTR_relative_rate.png',
        'post_finemapping/results/FINEMAP_UTR3_relative_rate.png',
        'post_finemapping/results/FINEMAP_UTR5_relative_rate.png',
        'post_finemapping/results/FINEMAP_any_stalling_struture_relative_rate.png',
        'post_finemapping/results/FINEMAP_eSTR_relative_rate.png',
        'post_finemapping/results/FINEMAP_exonic_relative_rate.png',
        'post_finemapping/results/FINEMAP_intergenic_relative_rate.png',
        'post_finemapping/results/FINEMAP_intronic_relative_rate.png',
        'post_finemapping/results/FINEMAP_period_is_1_relative_rate.png',
        'post_finemapping/results/FINEMAP_period_is_2_relative_rate.png',
        'post_finemapping/results/FINEMAP_period_is_3_relative_rate.png',
        'post_finemapping/results/FINEMAP_period_is_4_relative_rate.png',
        'post_finemapping/results/FINEMAP_period_is_5_relative_rate.png',
        'post_finemapping/results/FINEMAP_period_is_6_relative_rate.png',
        'post_finemapping/results/FINEMAP_promoter_relative_rate.png',
        'post_finemapping/results/FINEMAP_structure_is_HAIRP_relative_rate.png',
        'post_finemapping/results/FINEMAP_structure_is_IMOT_relative_rate.png',
        'post_finemapping/results/FINEMAP_structure_is_QUAD_relative_rate.png',
        'post_finemapping/results/FINEMAP_transcribed_non_protein_relative_rate.png',
        'post_finemapping/results/FINEMAP_unit_is_A_relative_rate.png',
        'post_finemapping/results/FINEMAP_unit_is_AC_relative_rate.png',
        'post_finemapping/results/FINEMAP_unit_is_AT_relative_rate.png',
        'post_finemapping/results/FINEMAP_unit_is_AG_relative_rate.png',
        'post_finemapping/results/FINEMAP_unit_is_AAT_relative_rate.png',
        'post_finemapping/results/FINEMAP_unit_is_AAC_relative_rate.png',
        'post_finemapping/results/FINEMAP_unit_is_AAAT_relative_rate.png',
        'post_finemapping/results/FINEMAP_unit_is_AAAC_relative_rate.png',
        'post_finemapping/results/FINEMAP_unit_is_AGAC_relative_rate.png',
        'post_finemapping/results/FINEMAP_unit_is_AAAG_relative_rate.png',
        'post_finemapping/results/FINEMAP_unit_is_AAGG_relative_rate.png',
        'post_finemapping/results/FINEMAP_unit_is_AATG_relative_rate.png',
        'post_finemapping/results/FINEMAP_prob_counts.png',
        'post_finemapping/results/FINEMAP_FM_eSTR_relative_rate.svg',
        'post_finemapping/results/FINEMAP_UTR3_relative_rate.svg',
        'post_finemapping/results/FINEMAP_UTR5_relative_rate.svg',
        'post_finemapping/results/FINEMAP_any_stalling_struture_relative_rate.svg',
        'post_finemapping/results/FINEMAP_eSTR_relative_rate.svg',
        'post_finemapping/results/FINEMAP_exonic_relative_rate.svg',
        'post_finemapping/results/FINEMAP_intergenic_relative_rate.svg',
        'post_finemapping/results/FINEMAP_intronic_relative_rate.svg',
        'post_finemapping/results/FINEMAP_period_is_1_relative_rate.svg',
        'post_finemapping/results/FINEMAP_period_is_2_relative_rate.svg',
        'post_finemapping/results/FINEMAP_period_is_3_relative_rate.svg',
        'post_finemapping/results/FINEMAP_period_is_4_relative_rate.svg',
        'post_finemapping/results/FINEMAP_period_is_5_relative_rate.svg',
        'post_finemapping/results/FINEMAP_period_is_6_relative_rate.svg',
        'post_finemapping/results/FINEMAP_prob_counts.svg',
        'post_finemapping/results/FINEMAP_promoter_relative_rate.svg',
        'post_finemapping/results/FINEMAP_structure_is_HAIRP_relative_rate.svg',
        'post_finemapping/results/FINEMAP_structure_is_IMOT_relative_rate.svg',
        'post_finemapping/results/FINEMAP_structure_is_QUAD_relative_rate.svg',
        'post_finemapping/results/FINEMAP_transcribed_non_protein_relative_rate.svg',
        'post_finemapping/results/FINEMAP_unit_is_A_relative_rate.svg',
        'post_finemapping/results/FINEMAP_unit_is_C_relative_rate.svg',
        'post_finemapping/results/FINEMAP_unit_is_AC_relative_rate.svg',
        'post_finemapping/results/FINEMAP_unit_is_AT_relative_rate.svg',
        'post_finemapping/results/FINEMAP_unit_is_AG_relative_rate.svg',
        'post_finemapping/results/FINEMAP_unit_is_AAT_relative_rate.svg',
        'post_finemapping/results/FINEMAP_unit_is_AAC_relative_rate.svg',
        'post_finemapping/results/FINEMAP_unit_is_AAAT_relative_rate.svg',
        'post_finemapping/results/FINEMAP_unit_is_AAAC_relative_rate.svg',
        'post_finemapping/results/FINEMAP_unit_is_AGAC_relative_rate.svg',
        'post_finemapping/results/FINEMAP_unit_is_AAAG_relative_rate.svg',
        'post_finemapping/results/FINEMAP_unit_is_AAGG_relative_rate.svg',
        'post_finemapping/results/FINEMAP_unit_is_AATG_relative_rate.svg',
    resources:
        time='2:00:00'
    shell:
        './post_finemapping/enrichments.py {phenotypes.phenotypes_in_use} ' 

checkpoint summary_table:
    input:
        'workflow/phenotypes.py',
        'snpstr/repeat_units.tab',
        chr_expand('side_analyses/str_annotations/closest_gene/chrCHR.tab'),
        chr_expand('side_analyses/str_annotations/closest_exon/chrCHR.tab'),
        chr_expand('side_analyses/str_annotations/nearby_exon_d_1000/chrCHR.tab'),
        chr_expand('side_analyses/str_annotations/nearby_gene_d_100000/chrCHR.tab'),
        chr_expand('side_analyses/str_annotations/intersects_gene/chrCHR.tab'),
        chr_expand('side_analyses/str_annotations/intersects_exon/chrCHR.tab'),
        chr_expand('side_analyses/str_annotations/intersects_CDS/chrCHR.tab'),
        chr_expand('side_analyses/str_annotations/intersects_five_prime_UTR/chrCHR.tab'),
        chr_expand('side_analyses/str_annotations/intersects_three_prime_UTR/chrCHR.tab'),
        chr_expand('side_analyses/str_annotations/intersects_UTR/chrCHR.tab'),
        chr_expand('side_analyses/str_annotations/intersects_transcript/chrCHR.tab'),
        results = expand(rules.concatenate_my_str_gwas.output.results, suffix='', phenotype='{phenotype}'),
        all_finemap_STR_contribs = rules.summarize_finemap_output.output.all_STR_contribs,
        susie_results = rules.summarize_susie_output.output.table,
    output:
        readme = 'finemapping/summary/{phenotype}_table_README.txt',
        table = 'finemapping/summary/{phenotype}_table.tab',
    resources:
        time='05:00:00',
        mem_gb = 75
    run:
        STRs = ''
        URLs = ''
        first = True
        for STR, url in phenotypes.pheno_descs[wildcards.phenotype].previous_STR_findings:
            if not first:
                STRs += ' '
                URLs += ' '
            first = False
            STRs += STR
            URLs += url

        exciting_STRs = ''
        first = True
        for STR in phenotypes.pheno_descs[wildcards.phenotype].exciting_STR_hits:
            if not first:
                exciting_STRs += ' '
            first = False
            exciting_STRs += STR

        command = (
            'finemapping/collate_strong_associations.py '
            f'{wildcards.phenotype} '
            f"'{phenotypes.pheno_descs[wildcards.phenotype].unit}' "
            f'{output.readme} '
            f'{output.table} '
            f'{input.results} '
            f'{input.all_finemap_STR_contribs} '
            f'{input.susie_results} '
        )
        if STRs:
            command += ' --previous-STR-findings ' + STRs
            command += ' --previous-STR-finding-URLs ' + URLs
        if exciting_STRs:
            command += ' --cool-loci ' + exciting_STRs
        shell(command)

rule finemap_signal_strengths:
    input:
        rules.summary_table.output.table
    output:
        graph='finemapping/finemap_results/{phenotype}/summary/signal_strengths.html'
    resources:
        time='00:30:00',
        mem_gb = 50
    shell:
        'finemapping/plot_finemap_association_strengths.py {wildcards.phenotype}'

######################### Create Files to Export ########################

rule finemapping_concordance:
    input:
        rules.susie_all_phenotypes.input,
        rules.finemap_all_phenotypes.input
    output:
        cs_min_abs_corrs=expand('export_scripts/results/cs_min_abs_corrs.{ext}', ext={'svg', 'png'}),
        cses_per_region=expand('export_scripts/results/cses_per_region.{ext}', ext={'svg', 'png'}),
        finemap_hits_per_region=expand('export_scripts/results/finemap_hits_per_region.{ext}', ext={'svg', 'png'}),
    shell:
        'export_scripts/finemapping_concordance.py {phenotypes.phenotypes_in_use}'
        

rule white_ld_heatmap:
    input:
        data=expand("array_imputed/pfile_converted/chr{chrom}.{ext}", chrom='{chrom}', ext={'pgen','psam','pvar'}),
        samples=expand(rules.subset_samples_for_phenotype.output.samples, phenotype='{phenotype}', ethnicity='white_brits', subpop='', subpop_prefix='')
    output:
        plink1_conversion=temp(expand(
            'post_finemapping/intermediate_results/plink1_chrom_regions/'
            'white_brits_{phenotype}_{chrom}_{start}_{end}_{str_pos}/data.{ext}',
            phenotype='{phenotype}',
            chrom='{chrom}',
            start='{start}',
            end='{end}',
            str_pos='{str_pos}',
            ext={'bed', 'bim', 'fam'}
        )),
        ld_file=temp(
            'post_finemapping/intermediate_results/plink1_chrom_regions/'
            'white_brits_{phenotype}_{chrom}_{start}_{end}_{str_pos}/ld.gz'
        ),
        variants=temp(
            'post_finemapping/intermediate_results/plink1_chrom_regions/'
            'white_brits_{phenotype}_{chrom}_{start}_{end}_{str_pos}/var_subset.txt'
        ),
        ld_heatmap='post_finemapping/results/ld_heatmap_white_brits__{phenotype}_{chrom}_{start}_{end}_{str_pos}.png'
    resources:
        time='1:00:00',
        mem_gb=40
    shell:
        'post_finemapping/run_my_haploview.sh '
        '{output.ld_heatmap} '
        'white_brits '
        '{wildcards.phenotype} '
        '{wildcards.chrom} '
        '{wildcards.start} '
        '{wildcards.end} '
        '{wildcards.str_pos} '


ruleorder: white_ld_heatmap > ethnic_ld_heatmap 
rule ethnic_ld_heatmap:
    input:
        data=expand("array_imputed/pfile_converted/chr{chrom}.{ext}", chrom='{chrom}', ext={'pgen','psam','pvar'}),
        samples=rules.subset_samples_for_phenotype.output.samples,
        variants=rules.white_ld_heatmap.output.variants
    output:
        plink1_conversion=temp(expand(
            'post_finemapping/intermediate_results/plink1_chrom_regions/'
            '{ethnicity}_{phenotype}_{chrom}_{start}_{end}_{str_pos}/data.{ext}',
            ethnicity='{ethnicity}',
            phenotype='{phenotype}',
            chrom='{chrom}',
            start='{start}',
            end='{end}',
            str_pos='{str_pos}',
            ext={'bed', 'bim', 'fam'}
        )),
        ld_file=temp(
            'post_finemapping/intermediate_results/plink1_chrom_regions/'
            '{ethnicity}_{phenotype}_{chrom}_{start}_{end}_{str_pos}/ld.gz'
        ),
        ld_heatmap='post_finemapping/results/ld_heatmap_{ethnicity}__{phenotype}_{chrom}_{start}_{end}_{str_pos}.png'
    resources:
        time='1:00:00',
        mem_gb=40
    shell:
        'post_finemapping/run_my_haploview.sh '
        '{output.ld_heatmap} '
        '{wildcards.ethnicity} '
        '{wildcards.phenotype} '
        '{wildcards.chrom} '
        '{wildcards.start} '
        '{wildcards.end} '
        '{wildcards.str_pos} '
        '{input.variants} '

rule region_ld_heatmap:
    input:
        rules.white_ld_heatmap.output.ld_heatmap,
        expand(
            rules.ethnic_ld_heatmap.output.ld_heatmap,
            ethnicity=other_ethnicities,
            phenotype='{phenotype}',
            chrom='{chrom}',
            start='{start}',
            end='{end}',
            str_pos='{str_pos}',
        )
    output:
        marker_file='post_finemapping/results/.ld_heatmap_{phenotype}_{chrom}_{start}_{end}_{str_pos}.done'
    resources:
        time='00:05:00'
    shell:
        'touch {output.marker_file}'

rule confusion_matrix:
    input:
        white_brits=expand(rules.subset_samples_for_phenotype.output.samples, ethnicity='white_brits', phenotype='no_phenotype', subpop='', subpop_prefix='')[0]
    output:
        png='export_scripts/results/{chrom}_{pos}_confusion_matrix.png',
        pdf='export_scripts/results/{chrom}_{pos}_confusion_matrix.pdf'
    resources:
        time='00:10:00'
    shell:
        'association/check_locus_imputation.py '
        '{wildcards.chrom} {wildcards.pos} {input.white_brits} {output.png} '

rule locus_summary_inputs:
    input:
        f'str_imputed/runs/{str_imputation_run_name}/vcfs/annotated_strs/chr{{chrom}}.vcf.gz',
        all_white_brits=expand(rules.filter_samples_for_ethnicity.output.samples, phenotype='no_phenotype', ethnicity='white_brits', subpop='', subpop_prefix='')
    output:
        'export_scripts/intermediate_results/chr{chrom}_loci_summary.tab'
    resources:
        time = '23:45:00',
        mem_gb=5
    shell:
        f'export_scripts/loci_stat_inputs.py {str_imputation_run_name} '
        '{wildcards.chrom} {input.all_white_brits} '

rule summarize_loci:
    input:
        [f'export_scripts/intermediate_results/chr{chrom}_loci_summary.tab' for chrom in range(1, 23)]
    output:
        expand('export_scripts/results/allele_count_thresh_{thresh}.{ext}',
                thresh={0.0004, 0.002, 0.01, 0.05},
                ext={'svg', 'png'}),
        expand('export_scripts/results/entropy_distribution.{ext}', ext={'svg', 'png'}),
        expand('export_scripts/results/multiallelicness_distribution.{ext}', ext={'svg', 'png'}),
        expand('export_scripts/results/heterozygosity_distribution.{ext}', ext={'svg', 'png'})
    resources:
        time='00:30:00',
        mem_gb=5
    shell:
        'export_scripts/loci_stats.py '

rule create_phenotypes_table:
    input:
        'workflow/phenotypes.py',
        expand(rules.filter_samples_for_ethnicity.output.samples, phenotype=phenotypes.phenotypes_in_use, ethnicity='white_brits', subpop='', subpop_prefix=''),
        pheno_expand('signals/peaks/PHENO_250000_5e-8.tab'),
        #pheno_expand('signals/comparison/PHENO.done') TODO one will fail, okay with that
    output:
        'export_scripts/results/phenotypes.tab',
        'export_scripts/results/phenotypes_README.txt'
    resources:
        time='00:02:00'
    shell:
        'export_scripts/make_pheno_table.py {phenotypes.phenotypes_in_use} '

def ethnicity_associations_readme_inputs():
    return {
        f'{ethnicity}__{pheno}':
        expand(rules.load_phenotype.output.readme, ethnicity=ethnicity, phenotype=pheno, subpop='', subpop_prefix='')[0]
        for pheno in phenotypes.phenotypes_in_use
        for ethnicity in other_ethnicities
    }

def ethnicity_associations_sample_inputs():
    return {
        f'{ethnicity}__{pheno}':
        expand(rules.subset_samples_for_phenotype.output.samples, ethnicity=ethnicity, phenotype=pheno, subpop='', subpop_prefix='')[0]
        for pheno in phenotypes.phenotypes_in_use
        for ethnicity in other_ethnicities
    }

rule document_ethnicity_associations:
    input:
        'workflow/phenotypes.py',
        *ethnicity_associations_readme_inputs().values(),
        *ethnicity_associations_sample_inputs().values()
    output:
        table=protected('export_scripts/results/ethnicity_associations.tab'),
        readme=protected('export_scripts/results/ethnicity_associations_README.txt')
    resources:
        time='00:10:00'
    run:
        with tempfile.NamedTemporaryFile(mode='w') as readmes, \
                tempfile.NamedTemporaryFile(mode='w') as samples:
            readmes.write(json.dumps(ethnicity_associations_readme_inputs()))
            readmes.write('\n')
            samples.write(json.dumps(ethnicity_associations_sample_inputs()))
            samples.write('\n')
            cmd = (
                'export_scripts/document_ethnicity_associations.py '
                '{readmes.name} '
                '{samples.name} ' 
            )
            shell(cmd)

rule concat_summary_tables:
    input:
        'workflow/phenotypes.py',
        tables=expand(rules.summary_table.output.table, phenotype=phenotypes.phenotypes_in_use)
    output:
        table='export_scripts/results/finemapping_summary_all_phenotypes.tab'
    resources:
        time = '00:10:00',
        mem_gb = 10
    run:
        cmd = 'export_scripts/concat_summary_tables.py {output[0]} '
        cmd += "'{" + json.dumps({phen: table for (phen, table) in zip(phenotypes.phenotypes_in_use, input.tables)}) + "}'"
        shell(cmd)

checkpoint concat_and_filter_summary_tables:
    input:
        'workflow/phenotypes.py',
        expand(rules.summary_table.output.table, phenotype=phenotypes.phenotypes_in_use)
    output:
        causal_STRs='post_finemapping/results/putatively_causal_STRs.tab',
        causal_STRs_README='post_finemapping/results/putatively_causal_STRs_README.txt',
        curated_STRs='post_finemapping/results/curated_STRs.tab',
        curated_STRs_README='post_finemapping/results/curated_STRs_README.txt'
    resources:
        time='00:10:00',
        mem_gb = 10
    run:
        cmd = 'post_finemapping/concat_and_filter_summary_tables.py '
        for phenotype in phenotypes.phenotypes_in_use:
            cmd += f'{phenotype} '
        shell(cmd)

def putatively_causal_spot_test_dict(hits_fname, checkpoint_to_get=None):
    if checkpoint_to_get is not None:
        checkpoint_to_get.get()
    hits = (pl.scan_csv(hits_fname, sep='\t')
        .join(
            pl.scan_csv('snpstr/flank_trimmed_vcf/vars.tab', sep='\t'),
            how='left',
            left_on=['chrom', 'start_pos'],
            right_on=['chrom', 'pos']
        ).select(['phenotype', 'chrom', 'snpstr_pos']).collect())
    return {
        f'{hits[idx, "phenotype"]}__{hits[idx, "chrom"]}__{hits[idx, "snpstr_pos"]}__{ethnicity}' :
        expand(rules.run_str_spot_test.output.result, phenotype=hits[idx, "phenotype"], chrom=hits[idx, "chrom"], pos=hits[idx, "snpstr_pos"], ethnicity=ethnicity)[0]
        for idx in range(hits.shape[0])
        for ethnicity in other_ethnicities
    }

rule validate_STRs_parent:
    params:
        spot_test_json_dict = lambda wildcards, input: json.dumps(putatively_causal_spot_test_dict(input.strs))
    resources:
        time='00:10:00'
    run:
        with tempfile.NamedTemporaryFile(mode='w') as fp:
            fp.write(params.spot_test_json_dict)
            fp.write('\n')
            cmd = 'post_finemapping/validate_hits.py {output.strs} {output.readme} {input.pos_to_snpstr_pos} {input.strs} {input.readme} {fp.name}'
            shell(cmd)

use rule validate_STRs_parent as validate_putatively_causal_STRs with:
    input:
        pos_to_snpstr_pos='snpstr/flank_trimmed_vcf/vars.tab',
        strs=rules.concat_and_filter_summary_tables.output.causal_STRs,
        readme=rules.concat_and_filter_summary_tables.output.causal_STRs_README,
        spot_tests=lambda wildcards: list(putatively_causal_spot_test_dict(rules.concat_and_filter_summary_tables.output.causal_STRs, checkpoints.concat_and_filter_summary_tables).values())
    output:
        strs='post_finemapping/results/validated/putatively_causal_STRs.tab',
        readme='post_finemapping/results/validated/putatively_causal_STRs_README.txt'

checkpoint exonic_finemapped_strs:
    input:
        'workflow/phenotypes.py',
        tables=expand(rules.summary_table.output.table, phenotype=phenotypes.phenotypes_in_use)
    output:
        table='post_finemapping/results/exonic_finemapped_STRs.tab',
        readme='post_finemapping/results/exonic_finemapped_STRs_README.txt'
    resources:
        time = '00:10:00',
        mem_gb = 2
    run:
        cmd = 'post_finemapping/exonic_finemapped_strs.py {output.table} {output.readme} '
        cmd += "'{" + json.dumps({phen: table for (phen, table) in zip(phenotypes.phenotypes_in_use, input.tables)}) + "}'"
        shell(cmd)

use rule validate_STRs_parent as validate_exonic_STRs with:
    input:
        pos_to_snpstr_pos='snpstr/flank_trimmed_vcf/vars.tab',
        strs=rules.exonic_finemapped_strs.output.table,
        readme=rules.exonic_finemapped_strs.output.readme,
        spot_tests=lambda wildcards: list(putatively_causal_spot_test_dict(rules.exonic_finemapped_strs.output.table, checkpoints.exonic_finemapped_strs).values())
    output:
        strs='post_finemapping/results/validated/exonic_finemapped_STRs.tab',
        readme='post_finemapping/results/validated/exonic_finemapped_STRs_README.txt'

# TODO double check these doesn't use subset_gwas_results files
rule assoc_figures:
    input:
        'association/plots/input/eosinophil_count/my_imputed_snp_chr21_results.tab',
        expand(rules.concatenate_linear_plink_gwas.output.results, phenotype=['eosinophil_count', 'total_bilirubin'], suffix=''),
    output:
        'export_scripts/results/validate_our_code.png',
        'export_scripts/results/panukbb_scatter.svg',
        'export_scripts/results/panukbb_scatter.png'
    resources:
        time='00:10:00',
        mem_gb=20
    shell:
        'export_scripts/assoc_figs.py'

rule peak_figs:
    input:
        'workflow/phenotypes.py',
        pheno_expand('signals/peaks/PHENO_250000_5e-8.tab')
    output:
        barplot=expand('export_scripts/results/peaks_by_pheno.{ext}', ext={'png', 'svg'}),
        heatmap=expand('export_scripts/results/peak_p_val_heatmap.{ext}', ext={'png', 'svg'})
    resources:
        time='01:00:00',
        mem_gb=5
    shell:
        f'export_scripts/signal_summary_images.py {" ".join(phenotypes.phenotypes_in_use)} '


rule replication_by_finemapper:
    input:
        hits=rules.validate_putatively_causal_STRs.output.strs
    output:
        susie='post_finemapping/results/replication_by_finemapper_susie.png',
        finemap='post_finemapping/results/replication_by_finemapper_finemap.png'
    resources:
        time='00:10:00',
        mem_gb=1
    shell:
        'post_finemapping/replication_by_finemapper.py {input.hits}'


def putatively_causal_regions():
    # should have a checkpoint get here, but don't cause slow
    causal_df = pl.scan_csv(
        rules.concat_and_filter_summary_tables.output.causal_STRs, sep='\t'
    )
    pheno_summaries = []
    for phenotype in phenotypes.phenotypes_in_use:
        pheno_summary = pl.scan_csv(
            expand(rules.summary_table.output.table, phenotype=phenotype)[0],
            sep='\t'
        ).with_column(
            pl.lit(phenotype).alias('phenotype')
        ).select([
            'phenotype', 'chrom', 'start_pos', 'signal_region'
        ])
        pheno_summaries.append(pheno_summary)
    causal_df = causal_df.join(
        pl.concat(pheno_summaries),
        how='left',
        on=['phenotype', 'chrom', 'start_pos']
    ).with_columns([
        pl.col('signal_region').str.extract('^([0-9]+)_([0-9]+)', 2).cast(int).alias('region_start'),
        pl.col('signal_region').str.extract('^(([0-9]+)_){2}([0-9]+)', 3).cast(int).alias('region_end'),
    ]).select([
        'phenotype', 'chrom', 'region_start', 'region_end'
    ]).filter(
        (pl.col('phenotype') != 'total_bilirubin') |
        (pl.col('chrom') != 12) |
        (pl.col('region_start') != 19976272)
    ).distinct().collect().to_dict(False)
            
    phenos, chroms, starts, ends = [
        causal_df[col] for col in ('phenotype', 'chrom', 'region_start', 'region_end')
    ]
    return list(zip(phenos, chroms, starts, ends))

'''
for pheno, chrom, start, end in putatively_causal_regions():
    dir_ = f'finemapping/susie_results/{pheno}/{chrom}_{start}_{end}'
    if os.path.exists(f'{dir_}/README.txt'):
        assert not os.path.exists(f'{dir_}/README.txt.normal_run')
        assert os.path.exists(f'{dir_}/colnames.txt')
        assert not os.path.exists(f'{dir_}/colnames.txt.normal_run')
        os.rename(f'{dir_}/README.txt', f'{dir_}/README.txt.normal_run')
        os.rename(f'{dir_}/colnames.txt', f'{dir_}/colnames.txt.normal_run')
'''

rule check_susie_region:
    output:
        txt='post_finemapping/intermediate_results/susie_var_discrep_{phenotype}_{chrom}_{start}_{end}.txt'
    resources:
        mem_gb=2,
        time='00:10:00'
    shell:
        'finemapping/susie_variant_discrepancy.py '
        '{wildcards.phenotype} '
        '{wildcards.chrom} '
        '{wildcards.start} '
        '{wildcards.end} '

rule check_susie_all:
    input:
        [expand(rules.check_susie_region.output.txt, phenotype=phenotype, chrom=chrom, start=start, end=end)[0]
         for phenotype, chrom, start, end in putatively_causal_regions()]

rule check_finemap_region:
    output:
        txt='post_finemapping/intermediate_results/finemap_var_discrep_{phenotype}_{chrom}_{start}_{end}.txt'
    resources:
        mem_gb=2,
        time='00:10:00'
    shell:
        'finemapping/finemap_variant_discrepancy.py '
        '{wildcards.phenotype} '
        '{wildcards.chrom} '
        '{wildcards.start} '
        '{wildcards.end} '

rule check_finemap_all:
    input:
        [expand(rules.check_finemap_region.output.txt, phenotype=phenotype, chrom=chrom, start=start, end=end)[0]
         for phenotype, chrom, start, end in putatively_causal_regions()]
        

rule susie_ratio_test_putatively_causal_results:
    input:
        #should use concat and filter summary tables as input, not because its a checkpoint which are insanely slow
        sum([
            expand(rules.susie_run_change_snp_str_ratio.output.alpha, ratio=4, phenotype=phenotype, chrom=chrom, start=start, end=end, PACSIN2='')
            for (phenotype, chrom, start, end) in putatively_causal_regions()
        ], [])

rule susie_hardcall_test_putatively_causal_results:
    input:
        #should use concat and filter summary tables as input, not because its a checkpoint which are insanely slow
        sum([
            expand(rules.susie_run.output.alpha, phenotype=phenotype, chrom=chrom, start=start, end=end, hardcalls='hardcall_', PACSIN2='')
            for (phenotype, chrom, start, end) in putatively_causal_regions()
            if (phenotype, chrom, end) not in {('total_bilirubin', 12, 22524428), ('alkaline_phosphatase', 1, 24309348)}
        ], [])


######################### Export Files ########################

rule graph_main_hits:
    input:
        snpstr_mapping='snpstr/flank_trimmed_vcf/vars.tab',
        hits=rules.validate_putatively_causal_STRs.output.strs,
    output:
        graph='post_finemapping/results/results_graph.{ext}'
    resources:
        time='00:30:00'
    shell:
        'post_finemapping/graph_main_hits.py '
        '{output.graph} '
        '{wildcards.ext} '
        '{input.hits} '
        '{input.snpstr_mapping} '

def link_file(input_, output_):
    input_ = '../'*output_.count('/') + input_
    shell(f"ln -s {input_} {output_}")

def link_files(inputs, outputs):
    for input_, output_ in zip(inputs, outputs):
        link_file(input_, output_)

rule export_phenotype_summaries:
    input:
        expand(rules.generate_peaks.output.peaks, zip,
            spacing=[250000, 500000, 2000000, 250000, 250000],
            thresh=['5e-8', '5e-8', '5e-8', '5e-9', '5e-10'],
            phenotype=['{phenotype}']*5),
        expand(rules.generate_peaks.output.readme, spacing=250000, thresh='5e-8', phenotype='{phenotype}'),
        rules.summary_table.output.readme,
        rules.summary_table.output.table,
        rules.finemap_signal_strengths.output.graph,
        expand(rules.manhattan_finemap.output.manhattan, binary_type='', ext='html', phenotype='{phenotype}'),
        rules.linear_overview_manhattan.output.manhattan
    output:
        # owns all symlinks in this directory
        complete='export/phenotypes/{phenotype}/.exported'
    resources:
        time='00:05:00',
        mem_gb=5 # got OOM error for this job, don't know why
    run:
        # delete all symlinks in the out directory before running
        out_dir = output.complete[:output.complete.rfind('/')] # strip the file off of the directory
        out_links = [path for path in pathlib.PosixPath(out_dir).iterdir() if path.is_symlink()]
        for path in out_links:
            path.unlink()

        for file_ in input:
            link_file(
                file_,
                f'export/phenotypes/{wildcards.phenotype}/' + 
                    file_.replace('/', '_').replace(f'_{wildcards.phenotype}', '')
            )

        shell(f"touch {output.complete}")

def phenotype_hit_highlights(wildcards):
    checkpoints.summary_table.get(phenotype=wildcards.phenotype)
    phenotype = wildcards.phenotype
    # TODO this should be settable: 'included_only_due_to_literature': float
    # TODO this should be settable: 'included_only_due_to_literature': curation 
    # TODO this should be inferred: 'association_p_value': float
    # comparison to string shouldn't cause casting
    # ask for dtype flag, not just dtypes?
    results = pl.read_csv(
        expand(rules.summary_table.output.table, phenotype=phenotype)[0],
        sep='\t',
        dtype={'alleles': str, 'association_p_value': float},
        null_values='NA'
    )
    results = results[
        (results['association_p_value'] <= 1e-10) &
        (
            results['mentioned_in_literature'] |
            results['curated'] |
            (results['FINEMAP_pcausal'] >= 0.8) |
            (results['SuSiE_CS_pcausal'] >= 0.33)
        )
    ]
    return (
        *expand(
            rules.manhattan_conditional.output.manhattan,
            zip,
            phenotype=[phenotype]*results.shape[0],
            chrom=results['chrom'],
            start=results['signal_region'].str.extract('^[^_]*_([^_]*)'),
            end=results['signal_region'].str.extract('^[^_]*_[^_]*_([^_]*)'),
            STRs='_' + results['SNPSTR_start_pos'].cast(str).to_numpy(),
            imputed_SNPs=['']*results.shape[0],
            ext=['png']*results.shape[0],
        ),
        *expand(
            rules.plot_white_brits_str_locus.output.png,
            zip,
            phenotype=[phenotype]*results.shape[0],
            chrom=results['chrom'],
            pos=results['SNPSTR_start_pos'],
            dosage_fraction_threshold=['0.001']*results.shape[0]
        ),
        *expand(
            rules.plot_white_brits_str_locus.output.svg,
            zip,
            phenotype=[phenotype]*results.shape[0],
            chrom=results['chrom'],
            pos=results['SNPSTR_start_pos'],
            dosage_fraction_threshold=['0.001']*results.shape[0]
        ),
        *expand(
            rules.confusion_matrix.output.png,
            zip,
            chrom=results['chrom'],
            pos=results['SNPSTR_start_pos'],
        ),
        *expand(
            rules.confusion_matrix.output.pdf,
            zip,
            chrom=results['chrom'],
            pos=results['SNPSTR_start_pos'],
        )
    )

rule export_phenotype_hit_highlights:
    input:
        phenotype_hit_highlights
    output:
        # owns all symlinks in this directory
        complete='export/phenotypes/{phenotype}/loci/.exported'
    resources:
        time='00:05:00'
    run:
        # delete all symlinks in the out directory before running
        out_dir = output.complete[:output.complete.rfind('/')] # strip the file off of the directory
        out_links = [path for path in pathlib.PosixPath(out_dir).iterdir() if path.is_symlink()]
        for path in out_links:
            path.unlink()

        # link new files
        for file_ in input:
            outfname = file_.split('/')[-1]
            outfname = re.sub(f'{wildcards.phenotype}(.|_)', '', outfname) 
            link_file(file_, f'{out_dir}/{outfname}')
        shell(f'touch {output.complete}')

def curated_hit_highlights(wildcards):
    files = []
    for phenotype in phenotypes.phenotypes_in_use:
        for loc in phenotypes.pheno_descs[phenotype].exciting_STR_hits:
            chrom, str_pos = [int(x) for x in loc.split(':')]
            files.append(expand(
                [rules.plot_white_brits_str_locus.output.svg, rules.plot_white_brits_str_locus.output.png],
                phenotype=phenotype,
                chrom=chrom,
                pos=str_pos,
                dosage_fraction_threshold='0.001'
            )[0])
            found_region = False

            for region_chrom, region_start, region_stop, in generated_regions(phenotype):
                if chrom == region_chrom and region_start <= str_pos <= region_stop:
                    files.append(expand(
                        rules.manhattan_conditional.output.manhattan,
                        phenotype=phenotype,
                        chrom=chrom,
                        start=region_start,
                        end=region_stop,
                        STRs='_' + str(str_pos),
                        imputed_SNPs='',
                        ext='png',
                    )[0])
                    found_region = True
                    break
            if not found_region:
                print(phenotype, chrom, str_pos)
                assert False
    return files

rule export_curated_hit_highlights:
    input:
        'workflow/phenotypes.py',
        hits=curated_hit_highlights
    output:
        # owns all symlinks in this directory
        complete='export/combined/curated_loci/.exported'
    resources:
        time='00:05:00'
    run:
        # delete all symlinks in the out directory before running
        out_dir = output.complete[:output.complete.rfind('/')] # strip the file off of the directory
        out_links = [path for path in pathlib.PosixPath(out_dir).iterdir() if path.is_symlink()]
        for path in out_links:
            path.unlink()

        # link new files
        for file_ in input.hits:
            link_file(file_, out_dir + '/' + file_.split('/')[-1])
        shell(f'touch {output.complete}')

inputs_for_export_combined = [
    'export_scripts/results/phenotypes.tab',
    'export_scripts/results/phenotypes_README.txt',
    rules.validate_putatively_causal_STRs.output.strs,
    rules.validate_putatively_causal_STRs.output.readme,
    'export_scripts/results/curated_STRs.tab',
    'export_scripts/results/curated_STRs_README.txt',
    rules.validate_exonic_STRs.output.strs,
    rules.validate_exonic_STRs.output.readme,
    'export_scripts/results/validate_our_code.png',
    'export_scripts/results/panukbb_scatter.png',
    'export_scripts/results/panukbb_scatter.svg',
    *rules.peak_figs.output.barplot,
    *rules.peak_figs.output.heatmap,
    'export_scripts/results/allele_count_thresh_0.001.png',
    'export_scripts/results/allele_count_thresh_0.001.svg',
    'export_scripts/results/allele_count_thresh_0.004.png',
    'export_scripts/results/allele_count_thresh_0.004.svg',
    'export_scripts/results/allele_count_thresh_0.015.png',
    'export_scripts/results/allele_count_thresh_0.015.svg',
    'export_scripts/results/allele_count_thresh_0.06.png',
    'export_scripts/results/allele_count_thresh_0.06.svg',
    'export_scripts/results/finemapping_summary_all_phenotypes.tab',
    *expand(rules.graph_main_hits.output.graph, ext={'html', 'svg'}),
    #*rules.enrichments.output
]

for fname in inputs_for_export_combined:
    if not isinstance(fname, str):
        print(f'input_for_export_combined {fname} is not a string!')
        assert False

rule export_combined_summaries_and_figures:
    input:
        inputs_for_export_combined
    output:
        ['export/combined/' + input_.split('/')[-1] for input_ in inputs_for_export_combined]
    resources:
        time='00:05:00'
    run:
        # delete all symlinks in the out directory before running
        out_links = [path for path in pathlib.PosixPath('export/combined').iterdir() if path.is_symlink()]
        for path in out_links:
            path.unlink()

        link_files(input, output)

rule export_all:
    input:
        'workflow/phenotypes.py',
        # temp stand in to make susie run
        # pheno_expand('finemapping/susie_results/PHENO/all_signals_done'),
        # --- per phenotype ---
        #expand(rules.export_phenotype_summaries.output.complete, phenotype=phenotypes.phenotypes_in_use),
        expand(rules.export_phenotype_hit_highlights.output.complete, phenotype=phenotypes.phenotypes_in_use),

        # -- combined ---
        'export/combined/curated_loci/.exported',
        # stand in for combined and figures 
        'export/combined/phenotypes.tab',

rule test:
    output:
        'foo'
    resources:
        time='00:50:00',
    run:
        print(resources.time)
        time.sleep(10)
        assert False

rule test2:
    output:
        bar='bar{asdf}'
    params:
        foo = lambda wildcards, output: print(output.bar)
