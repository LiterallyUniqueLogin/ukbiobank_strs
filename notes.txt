$UKB = path to this directory

Download UKBiobank genetics data:
instructions: https://biobank.ctsu.ox.ac.uk/crystal/crystal/docs/ukbgene_instruct.html
key: $UKB/.ukbkey
Our 'application' id with UKBiobank: 46122
script: $UKB/ukbgene
Two versions of data:
	v2 (directly measured variants, 6.6e5 variants, phased)
	v3 (imputed variants, 9.3e7 variants, unphased)
Two versions have same samples.
Download a hap file: ./ukbgene hap -cXX
	will have name ukb_hap_chrXX_v2.bgen
Corresponding sample file: ./ukbgene hap -cXX -m
	will have name ukb46122_hap_chrXX_v2_sNNN.sample where NNN is the number of samples remaining when downloaded (e.g. not quit the study yet)
Corresponding .bgi files (index files used by qctool and bgenix):
	http://biobank.ndph.ox.ac.uk/showcase/refer.cgi?id=1671
	Same name as bgen files except with a .bgi after the .bgen
Download an imp file: ./ukbgene imp -cXX
	will have name ukb_imp_chrXX_v3.bgen
Corresponding sample file: ./ukbgene imp -cXX -m
	will have name ukb46122_imp_chrXX_v3_sNNN.sample where NNN is the number of samples remaining when downloaded (e.g. not quit the study yet)
Corresponding .bgi files:
	http://biobank.ndph.ox.ac.uk/showcase/refer.cgi?id=1965
	Same name as bgen files except with a .bgi after the .bgen
The hap bgen files have the wrong ref vs alt order for some (but not all) alleles!
See below (in plink bgen to vcf conversion) for how to fix this

All markers in UKB are biallelic 
- cite:https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-018-0579-z/MediaObjects/41586_2018_579_MOESM1_ESM.pdf pg. 14

The bgen format is roughly equivalent to the vcf format in content, but is compressed.
plink2 (https://www.cog-genomics.org/plink/2.0/) supports the bgen format and is what I'm using. Note that it is in alpha
	and the author regularly responds quickly on biostars 
Tools qctool and bgenix both support the bgen format and are written by the authors of the bgen format, but are not as 
	reliably feature rich (and fast?) as plink
Dockerfile for environment which qctool and bgenix run in (on snorlax): /storage/jmargoliash/ukbiobank/qctool_bgenix_dockerfile
From that directory, building the Docker container: docker build -f qctool_bgenix_dockerfile -t qctool_test .
From $UKB, interactively running the Docker container:
	docker run --mount type=bind,source="$(pwd)",target=/ukbiobank -it  qctool_test:latest
Decided not to use these tools, plink2 processes bgen files just fine

Some UKB data is stored in EGA's servers instead of UKB's servers. To download it:
conda create -n ega
conda activate ega
conda config --add channels bioconda
ega install pyega3
wget https://ega-archive.org/files/CREDENTIALS_FILE
#modify the credentials file: remove the password line, change the email line
pyega3 -cf CREDENTIALS_FILE  datasets                                                                                   
pyega3 -cf CREDENTIALS_FILE files EGAD00010001497 > filesList.txt                                                       
#find the file I want                                                                                                   
pyega3 -cf CREDENTIALS_FILE fetch EGAF00001844707
Getting british white ancestry:
cut -f24 -d' ' ukb_sqc_v2.txt

To convert a ukbiobank bgen to vcf
plink2 --bgen file.bgen ref-first --sample file.sample --oxford-single-chr XX --export vcf --out outFile \
* outfile shouldn't have a .vcf
To fix the order of of ref vs alt alleles for hap files add the flag
	--ref-allele force $UKB/non_genetic_data/ukb_snp_bim/ukb_snp_chrI_v2.bim 5 2
To extract a random subset of e.g. 20000 samples, use the flag --thin-indiv-count 20000
To extract the same subset of samples as an already present vcf, head -n 6 file.vcf  | tail -n 1 | cut -f10-  > samples.txt
If the file you're sourcing from is a VCF already, that is enough (the sample ID's will match)
However, if the file you're sourcing from is bgen format, then you need to use VIM on the samples.txt file to 
	do the following replacements
* %s/\t/\r/g
* %s/_/ /g
This will generate a file that has two identical columns, one for the individual id and one for the family id,
	as they are both present but identical in this dataset
Then add the option --keep samples.txt to the plink2 command
NOTE plink2 will report only genotypes. Where that is not sufficient for filtering (e.g. imp dataset) 
	consider using qctool. (I've tried using the --vcf-dosage option, and it didn't change anything 
    i.e.  plink2 --bgen $UKB/original/imp/ukb_imp_chr17_v3.bgen ref-first --sample $UKB/original/imp/ukb*.sample --oxford-single-chr 17 --export vcf vcf-dosage=GP --out chr17_imp_1samp_GP --thin-indiv-count 1  )

To convert from bgen to vcf using qctool
cd $UKB
docker run --mount type=bind,source="$(pwd)",target=/ukbiobank -it  qctool_test:latest
qctool_v2.0.5 -g ../original/hap/ukb_hap_chr17_v2.bgen -s ../original/hap/ukb*sample 
	-og chr17_hap_1samp_qctool.vcf -incl-samples 1samp.sample
qctool outputs GP (genotype probbilities) instead of just GT like plink (raw genotypes)
Confirmed that running qctool on the haplotype dataset prodcues only GP with 0 or 1 probabilities
 - i.e. it provides no more information than plink. However, it does provide more information
 in the case of the imputed dataset.

To use get a list of variants from a bgen file:
* bgenix -g file.bgen -index
* bgenix -g file.bgen -list

To use qctool to get SNP summary statistics on hap bgen files:
* qctool -g file.bgen -threshold 0.9 -snp-stats -osnp output.file.txt
* (threshold would be problematic on imp files, not sure if not having it would cause error)
* Works at ~8 snps/sec on one core, no option for multicore, e.g. ~0.8 hr on chr17 in hap dataset. Too slow!
Better, use plink2 after conversion to vcf:
* plink2 --vcf file.vcf --freq --out out.file.name
* Took ~3min for chr9.

Plink timing tests (wall clock time, no replication for each test):
(Hap dataset chr9, so 29133 variants and 487409 samples)
* bgen to vcf conversion: 4m
* bgen to pfile conversion: 2.5m - max of 2 cores, mostly 1. Max of 11.5% of memory (=15G)
* pfile to vcf conversion: 3m10s - only 1 core, little memory
* vcf to frequencies calculation: 7.5m
* pfile to frequencies calculation: 10s
* cutting a 5k sample batch vcf from a vcf using cut: 4m
* cutting a 5k sample batch vcf from a pfile using plink: 8s
Conclusion: Convert to pfile, then convert to vcf files for use as necessary

To compress a vcf file:
bgzip -c -@ 20 < imp_temp.vcf > imp_temp.vcf.gz
To decompress, reverse the input/output and add the flag -d
To compress in place (e.g. delete the uncompressed file and create a compressed one)
bgzip -@ 20 imp_temp.vcf

To index a vcf file for use with bcftools or beagle:
tabix /storage/resources/datasets/Ukbiobank/hap_temp.vcf.gz
(must be run on a .gz file)
Note: the vcf file must have been compressed with bgzip, not gzip.
If compressed with gzip, uncompress then recompress with bgzip

Note: I have found that bedtools intersect is better than bcftools isec,
the below is only historical
To use bcftools to look at the variants present in one file A but not file B
bcftools isec -c none -n+2 fileA fileB > out.txt
	(Collapse: none means that variants must in addition to have the same reference position,
		also have exactly the same alleles)
Chr17 Results:
	711 (3.2%) variants in Hap not in SNPSTR out of 22215 in Hap and 776398 in SNPSTR
	imp is missing 16435 variants included in SNPSTR (that number includes STRs) according to bcftools isec
		only 1354 (0.17%) of these are single nucleotide substitutions/indels
		(cut bcftoolsOutput.txt -f3,4 | grep -ivP '([AGTC]{1,}\s[AGTC]{2,})|([ACTG]{2,}\s[ACTG]{1,})' | wc -l)

How to check for all multibp and multiallelic variants in a VCF: (add -v to grep to search for all singlebp, biallelic variants)
grep -iP '(\s[-AGTCyrwskmdvhbxn.,]{1,}\s[-AGTCyrwskmdvhbxn.,]{2,})|(\s[-AGTCyrwskmdvhbxn.,]{2,}\s[-AGTCyrwskmdvhbxn.,]{1,})' output2.txt > multibp_overlap.txt

Intersection different VCFs to see which loci they overlap in: bedtools intersect 
(even if they have different or multibasepair variants at those loci)
	cut -f1-9 $UKB/converted_vcf/hap/chr9.vcf > chr9_ukb_hap_loci_only.vcf
	zcat $UKB/SNPSTR/1kg.snp.str.chr9.vcf.gz | cut -f1-9 > chr9_snpstr_loci_only.vcf
	bedtools intersect -a chr9_snpstr_loci_only.vcf -b chr9_ukb_hap_loci_only.vcf -wa -wb -sorted > chr9_output.txt
	cut chr9_output.txt -f1-5,10-14 > chr9_overlap.txt
	cut -f1,2,4,5 chr9_overlap.txt > chr9_overlap_snpstr.txt
	cut -f6,7,9,10 chr9_overlap.txt > chr9_overlap_ukbhap.txt
	diff chr9_overlap_snpstr.txt chr9_overlap_ukbhap.txt > chr9_overlap_diffs.txt

Download SNPSTR data:
http://gymreklab.com/2018/03/05/snpstr_imputation.html#Usage
Ref paper: https://www.nature.com/articles/s41467-018-06694-0
Both SNPSTR and Beagle recommend using Beagle's author's tool conform-gt
for making the sample data file conform to the reference data file. However,
this throws out all G/C and A/T SNPs. Instead, I've found the plink solution
with --ref-allele above (conforming the sample data vcf to the sample bims) to not throw anything out. 
TODO confirm this doesn't miss anything.

Tools for phasing imputed data:
* SHAPEIT3 - TODO waiting on access

Possible tools for STR imputation from reference panel (good review article: https://www.nature.com/articles/nrg2796 )
* Beagle is the only known tool to handle multiallelic loci
* Alternatives include IMPUTEv4 (used by UKBiobank people)
  Imputev4 doesn't have a useful paper reference, but says it is the same as IMPUTEv2 but faster
  IMPUTEv2 ref: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2689936/
* this compares itself against Beagle, fastPHASE and MaCH (which has become minimacv4)
* fastPHASE ref:http://stephenslab.uchicago.edu/assets/papers/Scheet2006.pdf
  fastPHASE compares itself against GERBIL (now part of GEVALT) and HaploBlock
  but both of those seem old
* minimac ref: https://www.nature.com/articles/ng.3656
  ref is for v3, can't find a ref for v4

Testing minimac:
* Download minimac3 to convert reference vcf to m3vcf (minimac3 specific format that minimac4 requires but cannot produce)
* couldn't compile minimac3 from source (C errors) but downloaded exectuable works 
	ftp://share.sph.umich.edu/minimac3/Minimac3Executable.tar.gz
* Convert: /storage/jmargoliash/ukbiobank/Minimac3/bin/Minimac3 --processReference --refHaps SNPSTR/1kg.snp.str.chr9.vcf.gz
* (see here about a possibly faster but less accurate method: https://groups.google.com/forum/embed/?place=forum/minimac4-help#!topic/minimac4-help/Pv068psVa-4 )
* Conversion output: 
 Number of Markers read from VCF File                : 1200509
 Number of Markers with more than Two Alleles        : 17658
 Number of Markers failing FILTER = PASS             : 0
 Number of Markers with inconsistent Ref/Alt Allele  : 0
 Number of Markers with duplicate ID:POS:REF:ALT     : 0
 Number of Insertions                                : 18003
 Number of Deletions                                 : 394

 Number of Markers to be Recorded                    : 1182851
* Notice that the last line is the difference of the first two lines, e.g. markers with more than two alleles
  have been omitted. Thus we can't use minimac.

Testing fastPHASE:
* fastPHASE's documentation ( http://scheet.org/code/fastphase_doc_1.4.pdf )
	specifies "for each SNP site, any 2 characters may be used for the two SNP alleleles"
	and "fastPHASE  does  not  explicitly  or  comprehensively  check  the  formatof  the  input  file"
	so it does not seem reliable enough to warrant usage

Testing IMPUTEv4: documentation (downloaded after being given permission) says "All alleles must be coded as 0 or 1."
 so this won't work for STRs

Running beagle to impute SNPSTR variants into a UKBiobank file.
java -Xmx80000m -jar beagle.version.jar ref=snpstrFile.vcf.gz gt=ukbiobankFile.vcf out=outFile
	map=$UKB/beagle_genetic_maps/plink.chr9.GRCh37.map impute=true gp=true
Note: Beagle only outputs variants in the reference file in the generated file, 
    any variants only in the input file are discarded.

Confirmed that beagle imputes per sample instead of sharing info between samples, and that it is deterministic:
Get two vcf files from the ukbiobank data: one with 100 samples at random and one with just one of those samples
Get output vcf files from running beagle on both of those files
zcat beagleOutput100.vcf.gz | cut -f1-10 > | cut -f-7,9- bealgeOutput1of100.vcf
zcat beagleOutput1.vcf.gz | cut-f-7,9- > beagleOutput1.vcf
(The last cuts cut out the summary statistics which will necessarily be different for different datasets,
	we just want to know if the individual samples have been given the same genotypes)
diff beagleOutput1.vcf beagleOutput1of100.vcf

Confirmed that Beagle outputs the same alleles in the same order as the input
reference panel. (Ran 
bcftools isec $UKB/snpstr/vcf_1_sample/chr3.vcf.gz \
                $UKB/str_imputed/hap_no_preqc/vcf_batches/chr3_samples_1_to_1000.vcf.gz \
                -c none -n=2 | wc -l
and compared result with $UKB/pre_imputation_qc/common_variants/counts.csv)

    Confirmed that Beagle outputs alternate alleles even if they're frequency is 0
    (confirmed that this is true even for STRs where Beagle can omit an allele and
    still have a variant to report: performed the search in vim
    /:[^,]\+,[^,]\+,[^,]\+,[^,]\+,[^,]\+,\(\(1\)\|\(0\.\)\)
    at line 1214 in file $UKB/str_imputed/hap_no_preqc/vcf_batches/chr3_samples_1_to_1000.vcf 
    to confirm that there are no samples in that vcf with the 6th allele of STR_911910)

|The following estimates seem to be wrong, see below |
v                                                    v
Imputation time on hap dataset on snorlax:
2000 Samples on Chr17 took 5min30sec (84 cpu min)
5000 Samples on Chr17 took 13min20sec (214 cpu min)
Seems roughly linear, giving a 500k/2k * 5min * 1hr/60min = 20.8hr 
	time frame for imputing STRs into hap chr17
	ignoring preprocessing steps (converting to indexed, bgzipped vcf files)
10k samples memory overloaded, trying to find the max

Imputation time on hap on tscc hotel 1node 4ppn (no specified memory)
200 samples is the max (250 fails due to out of memory)
Give java 10000M Xmx. From top, have 13G virtual mem, but cap out ~6G real mem.
200 samples takes <1min for cutting a sample and ~1m15sec real time for
running beagle (~4min across cpus)
This gives a cost of:
$0.025/SU * 1SU/1hr * 1hr/60min * 2*(1 + 4)min/200 samples/chrom * 22 chrom * 500,000 samples ~ $229

^                                                 ^
|These above estimates seem to be wrong, see below|
(Maybe those runs were being killed for other reasons?)
Anyway, email from tscc says hotel nodes are 64GB memory, 16 cores for 4GB mem/core
At 4 cores we get 16GB mem. Setting java Xmx to 15750m
This is enough to run 1750 samples at once (Beagle takes too much mem and gets
Java OOM exceptino at 2000 samples). (Note that my tests were never killed for
taking too much memory when I set java Xmx to higher than 16000m, don't know
why, but don't want to have jobs start getting killed randomly later)
However, testing suggests that 1000 samples leaves the best time/#samples
ratio. Data points: (Number of samples is the divisor, some data points tested
twice)
(2*60+21)/500 = 0.282
(3*60+2)/750 = 0.243
((3*60+48)/1000 + (4*60+10)/1000)/2 = 0.239
(5*60+2)/1250 = 0.242
(6*60+(42+47)/2)/1500  = 0.270
(8*60+50)/1750 = 0.303

Estimating total cost: (assuming linearity with #samples and #variants and
extrapolability across chromosomes)
All the above tests were for chr22.
chr22 overlapping variants/total overlapping variants = 10531/641582 
#hap samples = 487409 
Cost = 641582 variants * 487409 samples * (0.239 seconds/(sample * 10531 variants)) * 1hr/3600sec * $0.025/hr
= $49

Beagle phasing is too slow: output sasy ~19 hrs/ window (window size is 40MB) for 20k samples with full access to snorlax.
This means 9GB/40MB * 19hrs * 500/20 -> 4453 days of compute time on Snorlax for the full genome.

Merging runtime tests:
Running bcftools merge on all the files at once (one process only, no parallelism) seemed to take too long. I remember
estimating 10days for chr3. Unfortunately, I didn't write down exactly the computation I did 
to produce that number.
Running bcftools merge in parallel on different subregions of the same chromosome (so that the results from all
these files could simply be concatenated) produced the errors:
    bcftools: thread_pool.c:552: tpool_worker: Assertion `j->p == p' failed.
    /var/spool/torque/mom_priv/jobs/17163768.tscc-mgr.local.SC: line 32: 10439
    Aborted                 bcftools merge
    $UKB/str_imputed/hap_no_preqc/vcf_batches/chr${INPUT2}_sample*.vcf.gz -o
    $UKB/str_imputed/hap_no_preqc/vcf_batches/chr${INPUT2}_pos_${INPUT1}_to_$END.vcf.gz
    --threads 4 -O z -r $INPUT2:$INPUT1-$END
    real    1247m11.353s
    user    739m5.762s
    sys     15m40.164s
Didn't want to diagnose these errors, so I gave up

My script (merge_all.py) on chr 3 (hap_no_preqc) takes 22hr 15min to handle ~267436 variants which leads to 
0.3s per variant or 6.8 days for chr3

Comparing my script vs bcftools:
Merging 4 1000 samples files of chr3 took roughly identical time.
MyScript:
real    127m32.459s
user    123m55.827s
sys     1m28.156s

BCFTools: (one process)
real    112m17.094s
user    131m34.263s
sys     3m54.177s

Timing for merging all samples (488 files, 488k samples) for only 100k variants:
MyScript
real    183m23.035s
user    154m53.108s
sys     9m46.169s

BCFTools (one process) (about 4.5x slower)
real    810m27.210s
user    884m51.793s
sys     18m11.236s

(TODO Current run started at 3 TB)

I've validated my merge script on chr3, first 2000 variants, all samples:
the output vcf is valid (took 32hrs). I've also validated merging 4 files
(first 4000 samples), all variants (took 55 hours). (Verified using the
validate.pbs script)

Note: bcftools merge will rename variants to shorter names if it thinks that
is appropriate. Don't know how to turn this feature off

Confirmed that (after merging the first 4 files/4000 samples)
the first non-str variants (1163 variants) and the header line (aside from the info
fields) are identical between my script and bcftools merge. Also confirmed
that except for bcftools renaming our str alleles, the first str allele line
is the same between my script and bcftools merge. Also verified that 
after merging all the files, the first 1163 and the header line are the 
same between the two. (Verified using the tests/check_for_differences scripts)

confirming that merging resumption works:
bcftools view -H -O v -r 3:43543023-43548317 -o $UKB/str_imputed/hap_no_preqc/tests/chr3_resumption_region_1.vcf $UKB/str_imputed/hap_no_preqc/vcfs/chr3.vcf.gz
bcftools merge -O v -r 3:43543023-43548317 -o $UKB/str_imputed/hap_no_preqc/tests/chr3_resumption_region_1_comparison.vcf $UKB/str_imputed/hap_no_preqc/vcf_batches/*sample*.vcf.gz
#The above merge command doesn't work because the samples are not in order, need to fix
#by manually adding the batch files to the command in a loop
#The other resumption region is contained in 3:56065375-56089822 
Confirmed!

After merging: compressing and indexing:
Compressing chr3 took 15.25 hours
Indexing chr3 took 8.45 hours
Compressed chr3 was 20% smaller than the sum of the sizes of 
all the individually compressed batches of chr3


VCF QC
TODO comparing VCFs (overlapping Sites, what they call STRs, multiallelic sites)
handle overlapping sites somehow?
TODO I'm assuming that beagle isn't smart enough to handle overlapping variants, so we should go through filtered_output
and remove all the overlapping variants from the input VCF file so we can impute the STRs into them, right?

TODO Sample QC/filtering pre (possible phasing and) imputation
(QC info about directly genotyped SNPs: http://biobank.ndph.ox.ac.uk/showcase/refer.cgi?id=1955 )
(QC datasets from the UKBiobank paper can be downloaded at: http://biobank.ndph.ox.ac.uk/showcase/label.cgi?id=100313)
The paper ( https://www.nature.com/articles/s41586-018-0579-z#Sec9 ) describes
SNP QC under the methods section Marker-based quality control (and in the
supplement), but ALSO under the Haplotype estimation and genotype imputation
where they say how they filter the SNPs that were phased and thus
downloadable via ukbgene hap.
Description of issues with UKB SNP chips:
https://www.biorxiv.org/content/10.1101/696799v1.full
https://www.sciencedirect.com/science/article/pii/S0002929718304683

Sample filtering:
(Note: plink doesn't support multiple --keep or --remove flags simultaneously,
so have to merge all the various subsets together myself)

Make sure UKB already remove high missingness and unusual heterozygosity, inferred sex.
Make sure the to filter 652 people with non-standard sex chromosome karyotypes
Don't filter based on white/British yet

sample qc file is stored on EGA (see http://biobank.ctsu.ox.ac.uk/crystal/refer.cgi?id=664)
	this resource shows that the info we want is in the sample QC file
	(http://biobank.ndph.ox.ac.uk/showcase/refer.cgi?id=531 , also 
	http://biobank.ndph.ox.ac.uk/showcase/showcase/docs/ukb_genetic_data_description.txt)
	This indicates the data used to be on UKB's website (resource 534) (http://biobank.ndph.ox.ac.uk/showcase/showcase/misc/gene_resource.txt)

Unrelated subset (see supplement s 3.7 for how these were generated)
There are two different listings of relatedness I found:
A: The listing I'm going with
The sample_qc file ($UKB/non_genetic_data/EGA/ukb_sqc_v2.txt) contains fields:
excluded.from.kinship.inference (was not considered for kinship)
in.kinship.table (after considering for kinship, was found to have kin)
Described here https://biobank.ctsu.ox.ac.uk/crystal/refer.cgi?id=531

The list of kinship relations among the people in.kinship.table 
is $UKB/non_genetic_data/ukbgene/ukb46122_rel_s488282.dat
(downloaded using ./ukbgene rel  described here: 
https://biobank.ctsu.ox.ac.uk/crystal/crystal/docs/ukbgene_instruct.html#rel )
Everyone in this table (aside from the few rows with negative signs) is 3rd
degree related or more

The numbers in this subset add up to the numbers mentioned in the UKB
Nature paper supplement
( https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-018-0579-z/MediaObjects/41586_2018_579_MOESM1_ESM.pdf )

B: Not using this
Data field 22011 describes whoe is related, 22018 describes exclusions,
and 22012 and 22013 describe how people are related
(e.g. https://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=22011 )
However, the numbers from these fields don't match the supplementary text
(e.g. 1,592 people excluded instead of 977)
Moreover, people are excluded for mixed ancestral background whereas this
seems to be a problem that the supplement address and controls for.
These data fields are part of the Interim genotype release, and I'm guessing
that they've been obsoleted by the (presumptively) newer analyses in the (A)
listing.

I've confirmed that the pariticipants in the kinship table downloaded with
ukbgene are in fact a subset of the ppts said to be there in the sample_qc
file from EGA. (This makes sense, some people have withdrawn since the sample
qc file was generated)
Proof:
    cd $UKB/post_imputation_qc/sample_filtering/hap
    conda activate ukb_analysis
    python 
    >>> import list_from_sample_qc
    >>> list_from_sample_qc.produceListFromSampleQC('in_kinship_temp', True, 20)
    tail -n +2 in_kinship_temp.sample | cut -f1 -d' ' | grep -v - | sort > qc_kinship_list.txt
    cut -f1,2 -d' ' $UKB/non_genetic_data/ukbgene/ukb46122_rel_s488282.dat | tail \
     -n +2 | grep -v - | sed 's/ /\n/' | sort | uniq > ukb_rel_kinship_list.txt
    diff qc_kinship_list.txt ukb_rel_kinship_list.txt | vim -

Paper on how to filter samples to a maximally unrelated sample list:
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3770842/
I've verified the paper's claim that neither PLINK nor KING expose the
unrelatedness algorithm in a way that can use someone else's relatedness estimates
I'm using this paper's algorithm (PRIMUS)

I tried using the nature paper's suggested algorithm instead of PRIMUS
(igraph maximal_independent_vertex_sets, they suggested
the igraph R version, I've tried the python version) but it uses more than
full node's memory on TSCC and crashes. This seems to be because
it finds all maximal solutions, not just one, and uses an exact algorithm
for this NP hard problem instead of an approximate one
See the github history of the unrelated_individuals.py file
for the code that ran this test.

All UKB samples: 487409 
White brits:     408972 (16% loss)
High quality:    408894 (0.02% loss)
Unrelated:       344295 (16% loss)

SNP filtering:
Filter SNPs with <0.3 informativeness. (Same as UKB paper, see methods)
 INFO scores for SNPs can be downloaded from here: 
 http://biobank.ndph.ox.ac.uk/showcase/search.cgi?wot=3&srch=info&sta0=on&sta1=on&sta2=on&sta3=on&str0=on&str3=on&fit0=on&fit10=on&fit20=on&fit30=on&fvt11=on&fvt21=on&fvt22=on&fvt31=on&fvt41=on&fvt51=on&fvt61=on&fvt101=on
 (here: http://biobank.ndph.ox.ac.uk/showcase/search.cgi , select resource, search 'info')
 (Or <0.4? This is used by IBD paper https://www.nature.com/articles/nature22969.pdf)
 (IBD paper requires INFO>0.8 and MAF>0.1 for at least one variant in confidence sets
 containing only imputed genotypes)
 (e.g. their imputation calls are not very certain.)
 TODO: read this paper (https://bmcgenet.biomedcentral.com/articles/10.1186/s12863-015-0248-2) I found
   on how to appropriately use INFO scores
 TODO: this paper suggests not filtering by INFO scores but using a regression model that takes them into account
   https://www.nature.com/articles/nrg2796 (note that these authors on the IMPUTEv2 page seem to indicate that filtering
   is acceptable, not sure why the contradict themselves)
 TODO: are info scores just directly computed from the raw genotype probabilities (GP in the VCF?) If so,
   any reason to filter based on INFO scores instead of using the GP or filtering based on the GP?
Remove SNPs with major frequency discrepancies between SNP-STR ref panel and UKBiobank cohort. 
 Also correlation discrepancies. (Shubham thinks correlation discrepencies are overkill)
 (cite box 2: https://www.nature.com/articles/s41576-018-0016-z#Sec17 )
 Use KS test to compare allel frequencies distributions?

Want to see if filtering SNPs inside/near STRs improves imputation accuracy
(on the theory that those SNPs are not called well by arrays because the
probes only bind to certain STR alleles)
 - can test this in UKB exome
 - cannot test this in 1000G because 1000G calls rely in part on WGS data
 and so wouldn't have this bias. (See 1000G 2015 paper
(https://www.nature.com/articles/nature15393) extended data figure 1 
primary data box for confirmation of this)

Ideas for QCing post (possible phasing and ) imputation:
TODO
Sample filters that can be applied immediately:
Filter samples with across the board poor imputation quality
Filter samples with unusual heterozygosity
	(fraction of non-missing alleles called heterozygous, after correction for population structure)
Get ~350K subset of that the UKB paper used in their height analysis
 (white, British ancestry, unrelated, filtered for unusual heterozygosity patterns after population structure,
  filtered for highly missing genotype calls, inferred sex matches reported sex)
Any sample filters after all samples are done?

STR filters:
Run Hardy-Weinberg Equilibrium test for expected genotype distribution
Filter STR genotypes with too low a minor allele frequency
Possibly filter STR genotypes with too low an estimate correlation between their calls and the
	true calls? (Beagle output INFO field DR2)
	Reference for how DR2 is calculated
	(see Appendix 1: https://www.cell.com/ajhg/pdf/S0002-9297(09)00012-3.pdf 
	A Unified Approach to Genotype Imputationand Haplotype-Phase Inference
	for Large Data Sets of Trios and Unrelated Individuals)
Compare distribution of alleles to the reference data set. Compare correlations between
	STRs and their local surroundings likewise. These being highly divergeant can send
	red flags (box 2 https://www.nature.com/articles/s41576-018-0016-z#Sec24 )
Compare per-locus heterozygosities to other datasets (SSC, 1000G, GTEx)
	or more specifically: distributions of alleles similar to other datasets?
Remove STRs with sex specific call rates
Consider filtering STRs which overlap genotyped values and are different


Using 1000Genomes for testing filtering thresholds:
Location of precomputed hipstr calls on snorlax:
/storage/s1saini/manuscript_strsnp/fig3/hipstr.1kg
Location of raw data: /storage/resources/datasets/1000Genomes/

Notes from call with Shubham: 2019/11/22
Doesn't know the files in /storage/resources/datasets/1000Genomes
He used the files in /storage/resources/datasets/1000Genomes/phase3
He believes (confirm) those variant calls are generated from low coverage wgs
sequencing (see the wgs in the filenames in
ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ )
The generated hipstr calls are at
/storage/s1saini/manuscript_strsnp/fig3/hipstr.1kg
This is generated from high coverage WGS sequencing done by Illumina (not the
main 1000G project), and only for 50 EU samples.
Shubham hasn't seen the high coverage WGS sequencing for all the 1000G
samples that 1000G has released since then.
The hipstr calls are pre-filtering (~1.6m STRs)
The hipstr calls that have been integrated into the imputation panel
are post-filtering and good quality (~300k STRs)
Shubham recommends rerunning quality filters on STRs that are produced post
imputation. (e.g. allele frequency, (sample-wise?) heterozygosity, 
hardy weinberg, etc. (sex bias?))

On measuring STR length and dealing with nearby variants/internal impurities:
When HipSTR calls an STR, if it finds nearby variantes (e.g. 3 bp before the
start of the STR), it includes those in the definition of the variant.
Example:
Ref CATAAAAGGT Alt CATAAAAAGGT,CATAAAAAGAT,CATAATAAGGT - the first two alt alleles
have the same number of copies of the STR motif but a difference in a nearby SNP,
the third alt allele has an impurity in the middle of the STR. But if those
variants aren't present in the reads HipSTR is calling the STR from, then it
might report the variant as Ref AAAA Alt AAAAA, AATAA . 
Note that GangSTR only calls repeat length - it does not call impurities or
nearby variants. 
For my analyses, I'm going to only focus on STR length, not on the nearby
variants. So a call of Startpos=7 CATAAAAGGT in one dataset is equivalent to
the call of Startpos=10 AAAA in another dataset, and both would contribute to
the bucket len=4 when doing anlaysis of the length versus a phenotype of
interest. However, we would remove any call AATAA from the dataset due to its
impurity. (I can compare calls for equivalence between datasets by finding the
recorded start position of that STR in the reference genome used by both
analyses, and then only looking at that point forward for different reads.
E.g. if the recorded start position is 10, then I would split 7 CATAAAAGGT
as CAT|AAAAGGT and counting four A's to the right of the split).
I should note in the paper that 
* we're only considering STR length
* we're ignoring any nearby SNPs
* we're filtering alleles called with internal impurities
This also impacts assocation testing:
if length is monotonically associated with phenotype of a number of different
genotypes, then there is some indication that the length is actually causal.
If length seems to not effect the phenotype except at a specific genotype,
then it is possible that that genotype just tags a nearby variant and the
length itself isn't actually causal

TODO
Check if I'm correctly distinguishing between phased and unphased calls in
1000G analysis

Compare beagle output hap vs imp
Find a blood trait gene to use a model
Try using a different phasing tool than beagle
Do my own imputation of SNPs using beagle, followed by imputation of STRs
Do my own imputation of SNPs using another tool (e.g. ShapeIt), followed by imputation of STRs
Confirm that other common imputation tools don't handle multiallelic variants
Find a way to confirm results before parallelizing
Parallelize

Look into imputation methods that can use the information percent
of imputed alleles (e.g. most of the variants we're basing the STR imputations off of
are themselves  imputed, and those will have an imputation information <1,
where imputation alpha means that this call has the usefulness of alpha true calls)

How does IMPUTE produce information percentages?

Use most likely genotype or estimated allele dosages for linear regression models?

TODO:
Compare that alt alleles are the same across batches
Compare that genotypes are the same before and after merging 
Is BCFTools subsample faster than cut?

TODO:
Look at fine mapping methods: https://www.nature.com/articles/s41576-018-0016-z/tables/1
UKB uses UK10K and all of 1000G when doing imputation, compared to just ~2.5K used in SNPSTR.

Follow up after fine mapping methods would be to do trans-ethnic fine mapping to improve
power for the loci which seem of interest
(Cite last bullet point box 2: https://www.nature.com/articles/s41576-018-0016-z#Sec17 )

Fine mapping:
From IBD paper https://www.nature.com/articles/nature22969 (according to their results, which have yet to be mechanistically confirmed)
 * 72% of loci harbor a single signal
 * 49% of variants are in a locus with a single signal
TODO signal == causal variant?

Figure out what percentage of loci target only a specific gene
 (coding variant >50% prob, or only gene in 50kb)
Figure out percentage of candidate genes still available

To download exome data, see here:
http://biobank.ndph.ox.ac.uk/showcase/refer.cgi?id=644
and here
http://biobank.ndph.ox.ac.uk/showcase/label.cgi?id=170
