# See https://cromwell.readthedocs.io/en/stable/Configuring/
# this configuration only accepts double quotes! not singule quotes
include required(classpath("application"))

system {
  abort-jobs-on-terminate = true
  io {
    number-of-requests = 30
    number-of-attempts = 5
    per = 1 second
  }
  file-hash-cache = true
  # increase the amount of attempts to write heartbeats before shutting down
  # by 6x. This seems like a self imposed crashing mechanism ...
  workflow-heartbeats {
    ttl = 15 minutes
    write-failure-shutdown-duration = 15 minutes
    heartbeat-interval = 1 minutes
  }

  # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist
  max-workflow-launch-count = 400 # default 50
  
  # Number of seconds between workflow launches
  new-workflow-poll-rate = 1 # default 20
  
  # Default number of cache read workers
  number-of-cache-read-workers = 1000 # default 25
}

# necessary for call result caching
# will need to stand up the MySQL server each time before running cromwell
# stand it up on the same node that's running cromwell
database {
  profile = "slick.jdbc.MySQLProfile$"
  db {
    driver = "com.mysql.cj.jdbc.Driver"
    url = "jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true"
    user = "root"
    password = "pass"
    connectionTimeout = 5000
  }
}

call-caching {
  enabled = true
  invalidate-bad-cache-results = true
}

docker {
  hash-lookup {
    enabled = true
    method = "remote"
  }
}

workflow-options {
  workflow-log-dir = "cromwell-workflow-logs"
  workflow-log-temporary = false
}

backend {
  # which backend do you want to use?
  # Right now I don't know how to choose this via command line, only here
  # default = "Local" # For running jobs on an interactive node
  default = "SLURM" # For running jobs by submitting them from an interactive node to the cluster
  providers {  
    # For running jobs on an interactive node
    Local {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        concurrent-job-limit = 10
        run-in-background = true
        root = "cromwell-executions"
        dockerRoot = "/cromwell-executions"
        runtime-attributes = """
          String? docker
        """
        submit = "/usr/bin/env bash ${script}"

        # We're asking bash-within-singularity to run the script, but the script's location on the machine
        # is different then the location its mounted to in the container, so need to change the path with sed
        submit-docker = """
          singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} bash \
               "$(echo ${script} | sed -e 's@.*cromwell-executions@/cromwell-executions@')"
        """
        filesystems {
          local {
            # hard-link by default because that works with docker (soft links don't)
            # and that avoids copying the file
            # use copy as a backup because any single file can only have 2^16 hard links
            # and this can be exceeded, causing workflows to perpetually fail from then on
            localization: ["hard-link", "copy"]
            caching {
              duplication-strategy: ["hard-link", "copy"]
               hashing-strategy: "fingerprint"
              fingerprint-size: 1048576 # 1 MB 
            }
          }
        }
      }
    }
    # For running jobs by submitting them from an interactive node to the cluster
    SLURM {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        concurrent-job-limit = 200
        root = "cromwell-executions"
        dockerRoot = "/cromwell-executions"

        runtime-attributes = """
          Int cpus = 1
          String dx_timeout
          String? docker
          Float memory
        """
        check-alive = "squeue -j ${job_id}"
        exit-code-timeout-seconds = 250
        job-id-regex = "Submitted batch job (\\d+).*"

        submit = """
          /expanse/projects/gymreklab/jmargoli/ukbiobank/workflow/submit.sh \
            sbatch \
            --account ddp268 \
            --partition ind-shared \
            --nodes 1 \
            --job-name=${job_name} \
            -o ${out} -e ${err}  \
            --mail-type FAIL --mail-user jonathan.margoliash@gmail.com \
            --ntasks-per-node=1 \
            --mem=$(python -c 'import math ; print(math.ceil(${memory}/1000000))') \
            -c ${cpus} \
            --time=$(echo ${dx_timeout} | sed -e 's/ //g' -e 's/\([0-9]\+\)h\([0-9]\+\)m/\1:\2:00/' -e 's/\([0-9]\+\)h/\1:00:00/' -e 's/\([0-9]\+\)m/\1:00/') \
            --chdir ${cwd} \
            --wrap "/bin/bash ${script}"
        """
        kill = "scancel ${job_id}"

        # We're asking bash-within-singularity to run the script, but the script's location on the machine
        # is different then the location its mounted to in the container, so need to change the path with sed
        submit-docker = """
          /expanse/projects/gymreklab/jmargoli/ukbiobank/workflow/submit.sh \
            sbatch \
            --account ddp268 \
            --partition ind-shared \
            --nodes 1 \
            --job-name=${job_name} \
            -o ${out} -e ${err}  \
            --mail-type FAIL --mail-user jonathan.margoliash@gmail.com \
            --ntasks-per-node=1 \
            --mem=$(python -c 'import math ; print(math.ceil(${memory}/1000000))') \
            -c ${cpus} \
            --time=$(echo ${dx_timeout} | sed -e 's/ //g' -e 's/\([0-9]\+\)h\([0-9]\+\)m/\1:\2:00/' -e 's/\([0-9]\+\)h/\1:00:00/' -e 's/\([0-9]\+\)m/\1:00/') \
            --chdir ${cwd} \
            --wrap "
              singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} bash \
                   \"$(echo ${script} | sed -e 's@.*cromwell-executions@/cromwell-executions@')\"
            "
        """
        kill-docker = "scancel ${job_id}"

        filesystems {
          local {
            # hard-link by default because that works with docker (soft links don't)
            # and that avoids copying the file
            # use copy as a backup because any single file can only have 2^16 hard links
            # and this can be exceeded, causing workflows to perpetually fail from then on
            localization: ["soft-link", "hard-link", "copy"]
            caching {
              duplication-strategy: ["hard-link", "copy"]
              hashing-strategy: "fingerprint"
              fingerprint-size: 1048576 # 1 MB 
            }
          }
        }
      }
    }
}}
