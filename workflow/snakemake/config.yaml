# config from https://github.com/jdblischak/smk-simple-slurm
# after reading https://github.com/Snakemake-Profiles/slurm/issues/40
# and https://github.com/Snakemake-Profiles/slurm/issues/73

# snakemake config should be `item: <value>`
# config for sbatch should be after the sbatch command in the form
# `--<option>=<value>
# snakemake vs sbatch options are not interchangeable
    
# snakemake replaces {rule} and {wildcards}
# see snakemake --help for --cluster
# this is then passed to slurm, which replaces %j with jobid
# https://support.pawsey.org.au/documentation/display/US/Customize+the+SLURM+output+file+name
# all the resources may be modified for different runs
# parsable needed for status-sacct

# This config is specific to running these analyses on the Expanse cluster
# managed by the San Diego Supercomputing Center
cluster:
  mkdir -p logs/{rule} &&
  sbatch
    --account ddp268
    --partition $(if [ -v PARTITION ] ; then echo $PARTITION ; else echo ind-shared ; fi)
    --export ALL
    --nodes 1
    --job-name={rule}-{wildcards}
    --output=logs/{rule}/{rule}-$(echo {wildcards} | sed -e 's_/_<slash>_g')-%j.out
    $(if (({resources.attempt} != 1)) ; then echo '--mail-type FAIL --mail-user jonathan.margoliash@gmail.com' ; fi)
    --ntasks-per-node={resources.threads}
    --mem={resources.mem_gb}G
    --time=$(if [ "$PARTITION" != debug ] ; then echo {resources.time} ; else echo "00:29:30" ; fi)
    --parsable
default-resources:
  - threads=1
  - mem_gb=2
  - attempt=0 # this doesn't correspond to the retry attempt. That needs to be manually set in the snakefile
  # every job must specify time in the 'hh:mm:ss' format (quotes required)
restart-times: 1
max-jobs-per-second: 30
max-status-checks-per-second: 10
latency-wait: 180
resources: mem_gb=15000
jobs: 500
keep-going: True
rerun-incomplete: True # https://groups.google.com/g/snakemake/c/fbQbnD8yYkQ?pli=1
printshellcmds: True
scheduler: greedy
use-conda: True
cluster-status: status-sacct.sh
