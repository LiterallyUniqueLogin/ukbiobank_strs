# See https://cromwell.readthedocs.io/en/stable/Configuring/
# this configuration only accepts double quotes! not singule quotes
include required(classpath("application"))

system {
  abort-jobs-on-terminate = true
  io {
    number-of-requests = 30
    per = 1 second
  }
  file-hash-cache = true
}

# necessary for call result caching
# will need to stand up the MySQL server each time before running cromwell
# stand it up on the same node that's running cromwell
database {
  profile = "slick.jdbc.MySQLProfile$"
  db {
    driver = "com.mysql.cj.jdbc.Driver"
    #url = "jdbc:mysql://localhost:3306/cromwell/?rewriteBatchedStatements=true&socketFactory=org.newsclub.net.mysql.AFUNIXDatabaseSocketFactory&junixsocket.file=/expanse/projects/gymreklab/jmargoli/ukbiobank/cromwell-executions/mysql_var_run_mysqld/mysqld.sock"
    #url = "jdbc:mysql:////?rewriteBatchedStatements=true&socketFactory=org.newsclub.net.mysql.AFUNIXDatabaseSocketFactory&junixsocket.file=/expanse/projects/gymreklab/jmargoli/ukbiobank/cromwell-executions/mysql_var_run_mysqld/mysqld.sock"
    #url = "jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true"
    # need to manually run mysql  -u root -P 3306 -h localhost -p --protocol tcp
    # create database cromwell ;
    # quit ;
    url = "jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true"
    user = "root"
    password = "pass"
    connectionTimeout = 5000
  }
}

### file based persistent database
# the implementation here proved to be poorly designed and so much too slow
#database {
#  profile = "slick.jdbc.HsqldbProfile$"
#  db {
#    driver = "org.hsqldb.jdbcDriver"
#    url = """
#    jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;
#    shutdown=false;
#    hsqldb.default_table_type=cached;hsqldb.tx=mvcc;
#    hsqldb.result_max_memory_rows=10000;
#    hsqldb.large_data=true;
#    hsqldb.applog=1;
#    hsqldb.lob_compressed=true;
#    hsqldb.script_format=3
#    """
#    connectionTimeout = 120000
#    numThreads = 1
#   }
#}

call-caching {
  enabled = true
  invalidate-bad-cache-results = true
}

# If you set the docker runtime attribute for a call
# then for call caching cromwell insists on trying to find
# the corresponding docker image and using its digest (i.e. hash code)
# as one of the keys for call caching (not just the docker string
# itself). If cromwell can't figure out how to locate the docker image
# then it simply refuses to try to load the call from cache
# this configures cromwell to try to look up the docker image digest
docker {
  hash-lookup {
    enabled = true
    method = "remote"
  }
}

backend {
  # which backend do you want to use?
  # Right now I don't know how to choose this via command line, only here
  #default = "Local" # For running jobs on an interactive node
  default = "SLURM" # For running jobs by submitting them from an interactive node to the cluster
  providers {  
    # For running jobs on an interactive node
    Local {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        concurrent-job-limit = 10
        run-in-background = true
        root = "cromwell-executions"
        dockerRoot = "/cromwell-executions"
        runtime-attributes = """
          String? docker
        """
        submit = "/usr/bin/env bash ${script}"
        # first make sure that the singularity image is on the disk and cached
        # this make take 10 min if the image isn't cached yet
        # use flock to make sure that this happens synchronously even if job
        # submission is asyncrhonous
        # use singularity exec instead of singularity pull to cache the image
        # because pull insists on writing the image to a location outside of the 
        # cache in addition to caching the image, so even if the image is cached
        # its not a no-op and is actually slower  (8s)
        # than booting up the image (1s) with exec, which also confirms the image
        # is cached
#          if [ -z $SINGULARITY_CACHEDIR ]; 
#            then CACHE_DIR=$HOME/.singularity/cache
#            else CACHE_DIR=$SINGULARITY_CACHEDIR
#          fi
#          mkdir -p $CACHE_DIR  
#          LOCK_FILE=$CACHE_DIR/singularity_pull_flock
#          flock --verbose --exclusive --timeout 900 $LOCK_FILE \
#          singularity exec --containall docker://${docker} echo "successfully pulled ${docker}!"
#          singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} bash -c \
#               "conda init bash && source ~/.bashrc && conda activate ukb \
#               && source $(echo ${script} | sed -e 's@.*cromwell-executions@/cromwell-executions@')"
             #  "$(cat ${script} | sed -e 's@.*cromwell-executions@/cromwell-executions@')"
        submit-docker = """
          echo "${script}"
          ls -lh "${script}"
          #cat ${script} | sed -e 's@.*cromwell-executions@/cromwell-executions@'
          singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} bash \
               "$(echo ${script} | sed -e 's@.*cromwell-executions@/cromwell-executions@')"
        """
        filesystems {
          local {
            localization: ["hard-link"]
            caching {
              duplication-strategy: ["hard-link"]
              hasing-strategy: "fingerprint"
              check-sibling-md5: true
              fingerprint-size: 1048576 # 1 MB 
            }
          }
        }
      }
    }
    # For running jobs by submitting them from an interactive node to the cluster
    SLURM {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        concurrent-job-limit = 500
        root = "cromwell-executions"
        dockerRoot = "/cromwell-executions"

        runtime-attributes = """
          Int cpus = 1
          String mem = "2g"
          String dx_timeout
          String? docker
        """
        check-alive = "squeue -j ${job_id}"
        exit-code-timeout-seconds = 500
        job-id-regex = "Submitted batch job (\\d+).*"

        submit = """
          sbatch \
            --account ddp268 \
            --partition ind-shared \
            --nodes 1 \
            --job-name=${job_name} \
            -o ${out} -e ${err}  \
            --mail-type FAIL --mail-user jonathan.margoliash@gmail.com \
            --ntasks-per-node=${cpus} \
            --mem=${mem} \
            -c ${cpus} \
            --time=$(echo ${dx_timeout} | sed -e 's/m/:00/' -e 's/h/:00:00/' -e 's/ //g') \
            --chdir ${cwd} \
            --wrap "/bin/bash ${script}"
        """
        kill = "scancel ${job_id}"

        # first make sure that the singularity image is on the disk and cached
        # this make take 10 min if the image isn't cached yet
        # use flock to make sure that this happens synchronously even if job
        # submission is asyncrhonous
        # use singularity exec instead of singularity pull to cache the image
        # because pull insists on writing the image to a location outside of the 
        # cache in addition to caching the image, so even if the image is cached
        # its not a no-op and is actually slower  (8s)
        # than booting up the image (1s) with exec, which also confirms the image
        # is cached
#          if [ -z $SINGULARITY_CACHEDIR ];
#            then CACHE_DIR=$HOME/.singularity/cache
#            else CACHE_DIR=$SINGULARITY_CACHEDIR
#          fi
#          mkdir -p $CACHE_DIR
#          LOCK_FILE=$CACHE_DIR/singularity_pull_flock
#          flock --verbose --exclusive --timeout 900 $LOCK_FILE \
#          singularity exec --containall docker://${docker} echo "successfully pulled ${docker}!"

#              singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} bash -c \
#                   \"conda init bash && source ~/.bashrc && conda activate ukb \
#                   && source $(echo ${script} | sed -e 's@.*cromwell-executions@/cromwell-executions@')\"
        submit-docker = """
          sbatch \
            --account ddp268 \
            --partition ind-shared \
            --nodes 1 \
            --job-name=${job_name} \
            -o ${out} -e ${err}  \
            --mail-type FAIL --mail-user jonathan.margoliash@gmail.com \
            --ntasks-per-node=${cpus} \
            --mem=${mem} \
            -c ${cpus} \
            --time=$(echo ${dx_timeout} | sed -e 's/m/:00/' -e 's/h/:00:00/' -e 's/ //g') \
            --chdir ${cwd} \
            --wrap "
              singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} bash \
                   \"$(echo ${script} | sed -e 's@.*cromwell-executions@/cromwell-executions@')\"
            "
        """
        kill-docker = "scancel ${job_id}"

        filesystems {
          local {
            localization: ["hard-link"]
            caching {
              duplication-strategy: ["hard-link"]
              check-sibling-md5: true
              hasing-strategy: "fingerprint"
              fingerprint-size: 1048576 # 1 MB 
            }
          }
        }

      }
    }
}}
